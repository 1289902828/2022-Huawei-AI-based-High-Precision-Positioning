{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "033fba83-aae5-46ad-b023-1f38b8d42550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "\ttorch.manual_seed(seed)  \n",
    "\tif torch.cuda.is_available(): \n",
    "\t\ttorch.cuda.manual_seed(seed)  # 为当前GPU设置\n",
    "\t\ttorch.cuda.manual_seed_all(seed)  # 为所有GPU设置\n",
    "\t#np.random.seed(seed) \n",
    "\ttorch.backends.cudnn.benchmark = False  \n",
    "\ttorch.backends.cudnn.deterministic = True  \n",
    "\n",
    "seed_everything(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c9ac234-2c85-4976-9629-de9db7d06269",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.0002\n",
    "TOTAL_EPOCHS = 1200\n",
    "split_ratio = 0.1\n",
    "change_learning_rate_epochs = 100\n",
    "\n",
    "model_save = '/hy-tmp/model/modelSubmit_20220927_1_v86.pth'\n",
    "\n",
    "DEVICE=torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "\tDEVICE=torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f803f9a-0858-413d-80f0-a123f9e99588",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\tdef __init__(self, trainX, trainY, split_ratio, mode='train'):\n",
    "\t\t\n",
    "\t\t#N = trainX.shape[0]\n",
    "\t\t\n",
    "\t\t#TrainNum = int((N*(1-split_ratio)))\n",
    "\t\t\n",
    "\t\tif mode=='train':\n",
    "\t\t\t#self.x = trainX[:TrainNum].astype(np.float32)\n",
    "\t\t\t#self.y = trainY[:TrainNum].astype(np.float32)\n",
    "\t\t\tself.x = trainX.astype(np.float32)\n",
    "\t\t\tself.y = trainY.astype(np.float32)\n",
    "\t\telse:\n",
    "\t\t\t#self.x = trainX[TrainNum:].astype(np.float32)\n",
    "\t\t\t#self.y = trainY[TrainNum:].astype(np.float32)\n",
    "\t\t\tself.x = trainX.astype(np.float32)\n",
    "\t\t\tself.y = trainY.astype(np.float32)\n",
    "\t\t\n",
    "\t\tself.len = len(self.y)\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.len\n",
    "\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\t\n",
    "\t\tx = self.x[idx]\n",
    "\t\ty = self.y[idx]\n",
    "\t\t\n",
    "\t\treturn (x, y)\n",
    "\n",
    "\n",
    "\n",
    "class Model_1(nn.Module):\n",
    "\tdef __init__(self, no_grad=True, infer_batchsize=5):\n",
    "\t\tsuper(Model_1, self).__init__()\n",
    "\t\t\n",
    "\t\tself.no_grad = no_grad\n",
    "\t\t\n",
    "\t\tself.infer_batchsize = infer_batchsize\n",
    "\t\t\n",
    "\t\tself.relu   = nn.LeakyReLU(negative_slope=0)\n",
    "\t\t\n",
    "\t\tself.conv1 = nn.Conv2d(2, 256, kernel_size = (3,2), stride = (2,2), padding= (1,0))\n",
    "\t\tself.conv2 = nn.Conv2d(256, 512, kernel_size = (3,2), stride = (2,2), padding= (1,0))\n",
    "\t\tself.conv3 = nn.Conv2d(512, 768, kernel_size = (3,1), stride = (2,1), padding= (1,0))\n",
    "\t\tself.conv4 = nn.Conv2d(768, 256, kernel_size = (3,1), stride = (2,1), padding= (1,0))\n",
    "\t\tself.conv5 = nn.Conv2d(256, 512, kernel_size = (3,1), stride = (2,1), padding= (1,0))\n",
    "\t\tself.conv6 = nn.Conv2d(512, 768, kernel_size = (3,1), stride = (2,1), padding= (1,0))\n",
    "\t\t#256，512，768，256，512，768\n",
    "\t\tself.bn1 = nn.BatchNorm2d(256)\n",
    "\t\tself.bn2 = nn.BatchNorm2d(512)\n",
    "\t\tself.bn3 = nn.BatchNorm2d(768)\n",
    "\t\tself.bn4 = nn.BatchNorm2d(256)\n",
    "\t\tself.bn5 = nn.BatchNorm2d(512)\n",
    "\t\tself.bn6 = nn.BatchNorm2d(768)\n",
    "\t\t\n",
    "\t\t#self.pool  = nn.MaxPool2d(kernel_size = (2,1), stride = (2,1), padding = 0)\n",
    "\t\tself.avgpool  = nn.AvgPool2d(kernel_size = (2,1), stride = (1,1), padding = (1,0))\n",
    "\t\t\n",
    "\t\tself.Flatten = nn.Flatten()\n",
    "\t\t\n",
    "\t\tself.fc_1  = nn.Linear(15360,2)\n",
    "\t\t#self.fc_2  = nn.Linear(1024,2)\n",
    "\t\n",
    "\tdef forward_with_grad(self, x):\n",
    "\t\t\n",
    "\t\t#取角落里四个点\n",
    "\t\tx1 = x[:,:,0:4,:]\n",
    "\t\tx2 = x[:,:,20:24,:]\n",
    "\t\tx3 = x[:,:,48:52,:]\n",
    "\t\tx4 = x[:,:,68:72,:]\n",
    "\t\tx = torch.cat([x1,x2,x3,x4], axis=2)\n",
    "\t\t\n",
    "\t\tx = x.permute(0,3,1,2)#[batch_size, 2, 256, 72]\n",
    "\t\t\n",
    "\t\tx = self.avgpool(x)\n",
    "\t\tx = self.avgpool(x)\n",
    "\t\t\n",
    "\t\tx = self.conv1(x)\n",
    "\t\tx = self.bn1(x)\n",
    "\t\tx = self.relu(x)\n",
    "\t\t#x = self.pool(x)\n",
    "\t\t\n",
    "\t\tx = self.conv2(x)\n",
    "\t\tx = self.bn2(x)\n",
    "\t\tx = self.relu(x)\n",
    "\t\t#x = self.pool(x)\n",
    "\t\t\n",
    "\t\tx = self.conv3(x)\n",
    "\t\tx = self.bn3(x)\n",
    "\t\tx = self.relu(x)\n",
    "\t\t#x = self.pool(x)\n",
    "\t\t\n",
    "\t\tx = self.conv4(x)\n",
    "\t\tx = self.bn4(x)\n",
    "\t\tx = self.relu(x)\n",
    "\t\t\n",
    "\t\tx = self.conv5(x)\n",
    "\t\tx = self.bn5(x)\n",
    "\t\tx = self.relu(x)\n",
    "\t\t\n",
    "\t\tx = self.conv6(x)\n",
    "\t\tx = self.bn6(x)\n",
    "\t\tx = self.relu(x)\n",
    "\t\t#print(x.shape)\n",
    "\t\t#[bs,2,5,18]\n",
    "\t\tx = self.Flatten(x)\n",
    "\t\t\n",
    "\t\tout = self.fc_1(x)\n",
    "\t\t\n",
    "\t\treturn out\n",
    "\t\t\n",
    "\t\n",
    "\tdef forward_without_grad(self, x):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t\n",
    "\t\t\t#取角落里四个点\n",
    "\t\t\tx1 = x[:,:,0:4,:]\n",
    "\t\t\tx2 = x[:,:,20:24,:]\n",
    "\t\t\tx3 = x[:,:,48:52,:]\n",
    "\t\t\tx4 = x[:,:,68:72,:]\n",
    "\t\t\tx = torch.cat([x1,x2,x3,x4], axis=2)\n",
    "\t\t\t\n",
    "\t\t\tx = x.permute(0,3,1,2)#[batch_size, 2, 256, 72]\n",
    "\t\t\t\n",
    "\t\t\tx = self.avgpool(x)\n",
    "\t\t\tx = self.avgpool(x)\n",
    "\t\t\t\n",
    "\t\t\tx = self.conv1(x)\n",
    "\t\t\tx = self.bn1(x)\n",
    "\t\t\tx = self.relu(x)\n",
    "\t\t\t#x = self.pool(x)\n",
    "\t\t\t\n",
    "\t\t\tx = self.conv2(x)\n",
    "\t\t\tx = self.bn2(x)\n",
    "\t\t\tx = self.relu(x)\n",
    "\t\t\t#x = self.pool(x)\n",
    "\t\t\t\n",
    "\t\t\tx = self.conv3(x)\n",
    "\t\t\tx = self.bn3(x)\n",
    "\t\t\tx = self.relu(x)\n",
    "\t\t\t#x = self.pool(x)\n",
    "\t\t\t\n",
    "\t\t\tx = self.conv4(x)\n",
    "\t\t\tx = self.bn4(x)\n",
    "\t\t\tx = self.relu(x)\n",
    "\t\t\t\n",
    "\t\t\tx = self.conv5(x)\n",
    "\t\t\tx = self.bn5(x)\n",
    "\t\t\tx = self.relu(x)\n",
    "\t\t\t\n",
    "\t\t\tx = self.conv6(x)\n",
    "\t\t\tx = self.bn6(x)\n",
    "\t\t\tx = self.relu(x)\n",
    "\t\t\t\n",
    "\t\t\tx = self.Flatten(x)\n",
    "\t\t\tout = self.fc_1(x)\n",
    "\t\t\t\n",
    "\t\t\treturn out\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\t\n",
    "\t\tif self.no_grad:\n",
    "\t\t\tb, _, _, _ = x.shape\n",
    "\t\t\tmini_batch_outs = []\n",
    "\t\t\tfor i in range(0, b, self.infer_batchsize):\n",
    "\t\t\t\t_out = self.forward_without_grad(x[i:i+self.infer_batchsize])\n",
    "\t\t\t\t#print(_out.shape)\n",
    "\t\t\t\tmini_batch_outs.append(_out)\n",
    "\t\t\t\n",
    "\t\t\tout = torch.cat(mini_batch_outs, axis=0)\n",
    "\t\telse:\n",
    "\t\t\tout = self.forward_with_grad(x)\n",
    "\t\t\t\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e1abcf2-33e2-4840-93bc-1debf4fba2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_1(\n",
      "  (relu): LeakyReLU(negative_slope=0)\n",
      "  (conv1): Conv2d(2, 256, kernel_size=(3, 2), stride=(2, 2), padding=(1, 0))\n",
      "  (conv2): Conv2d(256, 512, kernel_size=(3, 2), stride=(2, 2), padding=(1, 0))\n",
      "  (conv3): Conv2d(512, 768, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))\n",
      "  (conv4): Conv2d(768, 256, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))\n",
      "  (conv5): Conv2d(256, 512, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))\n",
      "  (conv6): Conv2d(512, 768, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))\n",
      "  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn6): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (avgpool): AvgPool2d(kernel_size=(2, 1), stride=(1, 1), padding=(1, 0))\n",
      "  (Flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc_1): Linear(in_features=15360, out_features=2, bias=True)\n",
      ")\n",
      "开始训练------\n",
      "Epoch : 1/1200, train_loss: 10.5562, test_loss: 2.0315, BestTest: 2.0315\n",
      "Epoch : 2/1200, train_loss: 1.7971, test_loss: 1.5743, BestTest: 1.5743\n",
      "Epoch : 3/1200, train_loss: 1.3636, test_loss: 1.4471, BestTest: 1.4471\n",
      "Epoch : 4/1200, train_loss: 1.2055, test_loss: 0.9147, BestTest: 0.9147\n",
      "Epoch : 5/1200, train_loss: 1.0612, test_loss: 0.7875, BestTest: 0.7875\n",
      "Epoch : 6/1200, train_loss: 1.0214, test_loss: 0.8100, BestTest: 0.7875\n",
      "Epoch : 7/1200, train_loss: 0.9201, test_loss: 0.9613, BestTest: 0.7875\n",
      "Epoch : 8/1200, train_loss: 0.9332, test_loss: 0.5900, BestTest: 0.5900\n",
      "Epoch : 9/1200, train_loss: 0.7969, test_loss: 0.6436, BestTest: 0.5900\n",
      "Epoch : 10/1200, train_loss: 0.8461, test_loss: 1.0062, BestTest: 0.5900\n",
      "Epoch : 11/1200, train_loss: 0.7609, test_loss: 0.7031, BestTest: 0.5900\n",
      "Epoch : 12/1200, train_loss: 0.6884, test_loss: 0.5550, BestTest: 0.5550\n",
      "Epoch : 13/1200, train_loss: 0.6948, test_loss: 0.6462, BestTest: 0.5550\n",
      "Epoch : 14/1200, train_loss: 0.6373, test_loss: 0.6291, BestTest: 0.5550\n",
      "Epoch : 15/1200, train_loss: 0.6368, test_loss: 0.4222, BestTest: 0.4222\n",
      "Epoch : 16/1200, train_loss: 0.5822, test_loss: 0.7733, BestTest: 0.4222\n",
      "Epoch : 17/1200, train_loss: 0.5835, test_loss: 0.4201, BestTest: 0.4201\n",
      "Epoch : 18/1200, train_loss: 0.5523, test_loss: 0.4907, BestTest: 0.4201\n",
      "Epoch : 19/1200, train_loss: 0.5358, test_loss: 0.4517, BestTest: 0.4201\n",
      "Epoch : 20/1200, train_loss: 0.4911, test_loss: 0.4240, BestTest: 0.4201\n",
      "Epoch : 21/1200, train_loss: 0.4891, test_loss: 0.3547, BestTest: 0.3547\n",
      "Epoch : 22/1200, train_loss: 0.4637, test_loss: 0.3993, BestTest: 0.3547\n",
      "Epoch : 23/1200, train_loss: 0.4599, test_loss: 0.4715, BestTest: 0.3547\n",
      "Epoch : 24/1200, train_loss: 0.4517, test_loss: 0.5987, BestTest: 0.3547\n",
      "Epoch : 25/1200, train_loss: 0.4428, test_loss: 0.4975, BestTest: 0.3547\n",
      "Epoch : 26/1200, train_loss: 0.4218, test_loss: 0.5105, BestTest: 0.3547\n",
      "Epoch : 27/1200, train_loss: 0.4306, test_loss: 0.4938, BestTest: 0.3547\n",
      "Epoch : 28/1200, train_loss: 0.4344, test_loss: 0.4193, BestTest: 0.3547\n",
      "Epoch : 29/1200, train_loss: 0.4097, test_loss: 0.4121, BestTest: 0.3547\n",
      "Epoch : 30/1200, train_loss: 0.4025, test_loss: 0.3524, BestTest: 0.3524\n",
      "Epoch : 31/1200, train_loss: 0.3906, test_loss: 0.5387, BestTest: 0.3524\n",
      "Epoch : 32/1200, train_loss: 0.3628, test_loss: 0.3662, BestTest: 0.3524\n",
      "Epoch : 33/1200, train_loss: 0.3889, test_loss: 0.3862, BestTest: 0.3524\n",
      "Epoch : 34/1200, train_loss: 0.3679, test_loss: 0.3151, BestTest: 0.3151\n",
      "Epoch : 35/1200, train_loss: 0.3416, test_loss: 0.3183, BestTest: 0.3151\n",
      "Epoch : 36/1200, train_loss: 0.3542, test_loss: 0.3362, BestTest: 0.3151\n",
      "Epoch : 37/1200, train_loss: 0.3755, test_loss: 0.3669, BestTest: 0.3151\n",
      "Epoch : 38/1200, train_loss: 0.3258, test_loss: 0.3355, BestTest: 0.3151\n",
      "Epoch : 39/1200, train_loss: 0.3498, test_loss: 0.4227, BestTest: 0.3151\n",
      "Epoch : 40/1200, train_loss: 0.3611, test_loss: 0.3496, BestTest: 0.3151\n",
      "Epoch : 41/1200, train_loss: 0.3348, test_loss: 0.2657, BestTest: 0.2657\n",
      "Epoch : 42/1200, train_loss: 0.3333, test_loss: 0.4169, BestTest: 0.2657\n",
      "Epoch : 43/1200, train_loss: 0.3425, test_loss: 0.3447, BestTest: 0.2657\n",
      "Epoch : 44/1200, train_loss: 0.3211, test_loss: 0.2963, BestTest: 0.2657\n",
      "Epoch : 45/1200, train_loss: 0.3203, test_loss: 0.4432, BestTest: 0.2657\n",
      "Epoch : 46/1200, train_loss: 0.3396, test_loss: 0.2767, BestTest: 0.2657\n",
      "Epoch : 47/1200, train_loss: 0.2968, test_loss: 0.2805, BestTest: 0.2657\n",
      "Epoch : 48/1200, train_loss: 0.2865, test_loss: 0.2277, BestTest: 0.2277\n",
      "Epoch : 49/1200, train_loss: 0.2934, test_loss: 0.3385, BestTest: 0.2277\n",
      "Epoch : 50/1200, train_loss: 0.2893, test_loss: 0.2197, BestTest: 0.2197\n",
      "Epoch : 51/1200, train_loss: 0.3002, test_loss: 0.3351, BestTest: 0.2197\n",
      "Epoch : 52/1200, train_loss: 0.3081, test_loss: 0.2773, BestTest: 0.2197\n",
      "Epoch : 53/1200, train_loss: 0.3007, test_loss: 0.2765, BestTest: 0.2197\n",
      "Epoch : 54/1200, train_loss: 0.2919, test_loss: 0.2716, BestTest: 0.2197\n",
      "Epoch : 55/1200, train_loss: 0.3067, test_loss: 0.2720, BestTest: 0.2197\n",
      "Epoch : 56/1200, train_loss: 0.2968, test_loss: 0.2478, BestTest: 0.2197\n",
      "Epoch : 57/1200, train_loss: 0.3017, test_loss: 0.4801, BestTest: 0.2197\n",
      "Epoch : 58/1200, train_loss: 0.3140, test_loss: 0.3318, BestTest: 0.2197\n",
      "Epoch : 59/1200, train_loss: 0.2845, test_loss: 0.3463, BestTest: 0.2197\n",
      "Epoch : 60/1200, train_loss: 0.2882, test_loss: 0.2444, BestTest: 0.2197\n",
      "Epoch : 61/1200, train_loss: 0.2685, test_loss: 0.1916, BestTest: 0.1916\n",
      "Epoch : 62/1200, train_loss: 0.2858, test_loss: 0.3207, BestTest: 0.1916\n",
      "Epoch : 63/1200, train_loss: 0.2649, test_loss: 0.2615, BestTest: 0.1916\n",
      "Epoch : 64/1200, train_loss: 0.2852, test_loss: 0.3635, BestTest: 0.1916\n",
      "Epoch : 65/1200, train_loss: 0.2618, test_loss: 0.2224, BestTest: 0.1916\n",
      "Epoch : 66/1200, train_loss: 0.2541, test_loss: 0.2141, BestTest: 0.1916\n",
      "Epoch : 67/1200, train_loss: 0.2681, test_loss: 0.3068, BestTest: 0.1916\n",
      "Epoch : 68/1200, train_loss: 0.2891, test_loss: 0.2000, BestTest: 0.1916\n",
      "Epoch : 69/1200, train_loss: 0.2658, test_loss: 0.2941, BestTest: 0.1916\n",
      "Epoch : 70/1200, train_loss: 0.2554, test_loss: 0.2530, BestTest: 0.1916\n",
      "Epoch : 71/1200, train_loss: 0.2660, test_loss: 0.2834, BestTest: 0.1916\n",
      "Epoch : 72/1200, train_loss: 0.2681, test_loss: 0.2007, BestTest: 0.1916\n",
      "Epoch : 73/1200, train_loss: 0.2464, test_loss: 0.2956, BestTest: 0.1916\n",
      "Epoch : 74/1200, train_loss: 0.2557, test_loss: 0.2471, BestTest: 0.1916\n",
      "Epoch : 75/1200, train_loss: 0.2487, test_loss: 0.2331, BestTest: 0.1916\n",
      "Epoch : 76/1200, train_loss: 0.2657, test_loss: 0.2266, BestTest: 0.1916\n",
      "Epoch : 77/1200, train_loss: 0.2483, test_loss: 0.2246, BestTest: 0.1916\n",
      "Epoch : 78/1200, train_loss: 0.2548, test_loss: 0.1765, BestTest: 0.1765\n",
      "Epoch : 79/1200, train_loss: 0.2435, test_loss: 0.3760, BestTest: 0.1765\n",
      "Epoch : 80/1200, train_loss: 0.2578, test_loss: 0.2865, BestTest: 0.1765\n",
      "Epoch : 81/1200, train_loss: 0.2332, test_loss: 0.2449, BestTest: 0.1765\n",
      "Epoch : 82/1200, train_loss: 0.2307, test_loss: 0.2101, BestTest: 0.1765\n",
      "Epoch : 83/1200, train_loss: 0.2429, test_loss: 0.3292, BestTest: 0.1765\n",
      "Epoch : 84/1200, train_loss: 0.2257, test_loss: 0.2195, BestTest: 0.1765\n",
      "Epoch : 85/1200, train_loss: 0.2363, test_loss: 0.3182, BestTest: 0.1765\n",
      "Epoch : 86/1200, train_loss: 0.2552, test_loss: 0.3002, BestTest: 0.1765\n",
      "Epoch : 87/1200, train_loss: 0.2335, test_loss: 0.2298, BestTest: 0.1765\n",
      "Epoch : 88/1200, train_loss: 0.2335, test_loss: 0.2511, BestTest: 0.1765\n",
      "Epoch : 89/1200, train_loss: 0.2321, test_loss: 0.3295, BestTest: 0.1765\n",
      "Epoch : 90/1200, train_loss: 0.2290, test_loss: 0.2156, BestTest: 0.1765\n",
      "Epoch : 91/1200, train_loss: 0.2263, test_loss: 0.1907, BestTest: 0.1765\n",
      "Epoch : 92/1200, train_loss: 0.2331, test_loss: 0.1784, BestTest: 0.1765\n",
      "Epoch : 93/1200, train_loss: 0.2181, test_loss: 0.1951, BestTest: 0.1765\n",
      "Epoch : 94/1200, train_loss: 0.2306, test_loss: 0.2097, BestTest: 0.1765\n",
      "Epoch : 95/1200, train_loss: 0.2227, test_loss: 0.2328, BestTest: 0.1765\n",
      "Epoch : 96/1200, train_loss: 0.2030, test_loss: 0.1963, BestTest: 0.1765\n",
      "Epoch : 97/1200, train_loss: 0.2304, test_loss: 0.2562, BestTest: 0.1765\n",
      "Epoch : 98/1200, train_loss: 0.2243, test_loss: 0.3048, BestTest: 0.1765\n",
      "Epoch : 99/1200, train_loss: 0.2103, test_loss: 0.1667, BestTest: 0.1667\n",
      "Epoch : 100/1200, train_loss: 0.1538, test_loss: 0.1130, BestTest: 0.1130\n",
      "Epoch : 101/1200, train_loss: 0.1446, test_loss: 0.1309, BestTest: 0.1130\n",
      "Epoch : 102/1200, train_loss: 0.1507, test_loss: 0.1460, BestTest: 0.1130\n",
      "Epoch : 103/1200, train_loss: 0.1480, test_loss: 0.1154, BestTest: 0.1130\n",
      "Epoch : 104/1200, train_loss: 0.1402, test_loss: 0.1192, BestTest: 0.1130\n",
      "Epoch : 105/1200, train_loss: 0.1585, test_loss: 0.1177, BestTest: 0.1130\n",
      "Epoch : 106/1200, train_loss: 0.1480, test_loss: 0.1196, BestTest: 0.1130\n",
      "Epoch : 107/1200, train_loss: 0.1410, test_loss: 0.1122, BestTest: 0.1122\n",
      "Epoch : 108/1200, train_loss: 0.1399, test_loss: 0.1188, BestTest: 0.1122\n",
      "Epoch : 109/1200, train_loss: 0.1487, test_loss: 0.1410, BestTest: 0.1122\n",
      "Epoch : 110/1200, train_loss: 0.1439, test_loss: 0.1380, BestTest: 0.1122\n",
      "Epoch : 111/1200, train_loss: 0.1475, test_loss: 0.1309, BestTest: 0.1122\n",
      "Epoch : 112/1200, train_loss: 0.1397, test_loss: 0.1114, BestTest: 0.1114\n",
      "Epoch : 113/1200, train_loss: 0.1406, test_loss: 0.1278, BestTest: 0.1114\n",
      "Epoch : 114/1200, train_loss: 0.1297, test_loss: 0.1046, BestTest: 0.1046\n",
      "Epoch : 115/1200, train_loss: 0.1381, test_loss: 0.1104, BestTest: 0.1046\n",
      "Epoch : 116/1200, train_loss: 0.1431, test_loss: 0.1249, BestTest: 0.1046\n",
      "Epoch : 117/1200, train_loss: 0.1335, test_loss: 0.1303, BestTest: 0.1046\n",
      "Epoch : 118/1200, train_loss: 0.1393, test_loss: 0.1232, BestTest: 0.1046\n",
      "Epoch : 119/1200, train_loss: 0.1336, test_loss: 0.1163, BestTest: 0.1046\n",
      "Epoch : 120/1200, train_loss: 0.1464, test_loss: 0.1098, BestTest: 0.1046\n",
      "Epoch : 121/1200, train_loss: 0.1301, test_loss: 0.1692, BestTest: 0.1046\n",
      "Epoch : 122/1200, train_loss: 0.1369, test_loss: 0.1175, BestTest: 0.1046\n",
      "Epoch : 123/1200, train_loss: 0.1300, test_loss: 0.1393, BestTest: 0.1046\n",
      "Epoch : 124/1200, train_loss: 0.1382, test_loss: 0.1357, BestTest: 0.1046\n",
      "Epoch : 125/1200, train_loss: 0.1349, test_loss: 0.0993, BestTest: 0.0993\n",
      "Epoch : 126/1200, train_loss: 0.1358, test_loss: 0.0912, BestTest: 0.0912\n",
      "Epoch : 127/1200, train_loss: 0.1316, test_loss: 0.1226, BestTest: 0.0912\n",
      "Epoch : 128/1200, train_loss: 0.1250, test_loss: 0.1362, BestTest: 0.0912\n",
      "Epoch : 129/1200, train_loss: 0.1230, test_loss: 0.1152, BestTest: 0.0912\n",
      "Epoch : 130/1200, train_loss: 0.1287, test_loss: 0.1212, BestTest: 0.0912\n",
      "Epoch : 131/1200, train_loss: 0.1259, test_loss: 0.1260, BestTest: 0.0912\n",
      "Epoch : 132/1200, train_loss: 0.1290, test_loss: 0.1433, BestTest: 0.0912\n",
      "Epoch : 133/1200, train_loss: 0.1251, test_loss: 0.1001, BestTest: 0.0912\n",
      "Epoch : 134/1200, train_loss: 0.1402, test_loss: 0.1529, BestTest: 0.0912\n",
      "Epoch : 135/1200, train_loss: 0.1335, test_loss: 0.1425, BestTest: 0.0912\n",
      "Epoch : 136/1200, train_loss: 0.1314, test_loss: 0.1419, BestTest: 0.0912\n",
      "Epoch : 137/1200, train_loss: 0.1323, test_loss: 0.1149, BestTest: 0.0912\n",
      "Epoch : 138/1200, train_loss: 0.1262, test_loss: 0.1138, BestTest: 0.0912\n",
      "Epoch : 139/1200, train_loss: 0.1257, test_loss: 0.1087, BestTest: 0.0912\n",
      "Epoch : 140/1200, train_loss: 0.1292, test_loss: 0.1371, BestTest: 0.0912\n",
      "Epoch : 141/1200, train_loss: 0.1331, test_loss: 0.1019, BestTest: 0.0912\n",
      "Epoch : 142/1200, train_loss: 0.1246, test_loss: 0.1441, BestTest: 0.0912\n",
      "Epoch : 143/1200, train_loss: 0.1260, test_loss: 0.1065, BestTest: 0.0912\n",
      "Epoch : 144/1200, train_loss: 0.1208, test_loss: 0.1166, BestTest: 0.0912\n",
      "Epoch : 145/1200, train_loss: 0.1245, test_loss: 0.0982, BestTest: 0.0912\n",
      "Epoch : 146/1200, train_loss: 0.1256, test_loss: 0.1016, BestTest: 0.0912\n",
      "Epoch : 147/1200, train_loss: 0.1195, test_loss: 0.1214, BestTest: 0.0912\n",
      "Epoch : 148/1200, train_loss: 0.1243, test_loss: 0.0793, BestTest: 0.0793\n",
      "Epoch : 149/1200, train_loss: 0.1258, test_loss: 0.1488, BestTest: 0.0793\n",
      "Epoch : 150/1200, train_loss: 0.1201, test_loss: 0.1223, BestTest: 0.0793\n",
      "Epoch : 151/1200, train_loss: 0.1181, test_loss: 0.0970, BestTest: 0.0793\n",
      "Epoch : 152/1200, train_loss: 0.1198, test_loss: 0.1316, BestTest: 0.0793\n",
      "Epoch : 153/1200, train_loss: 0.1227, test_loss: 0.0826, BestTest: 0.0793\n",
      "Epoch : 154/1200, train_loss: 0.1221, test_loss: 0.0984, BestTest: 0.0793\n",
      "Epoch : 155/1200, train_loss: 0.1233, test_loss: 0.1313, BestTest: 0.0793\n",
      "Epoch : 156/1200, train_loss: 0.1245, test_loss: 0.0942, BestTest: 0.0793\n",
      "Epoch : 157/1200, train_loss: 0.1214, test_loss: 0.1078, BestTest: 0.0793\n",
      "Epoch : 158/1200, train_loss: 0.1132, test_loss: 0.1224, BestTest: 0.0793\n",
      "Epoch : 159/1200, train_loss: 0.1185, test_loss: 0.1072, BestTest: 0.0793\n",
      "Epoch : 160/1200, train_loss: 0.1160, test_loss: 0.1066, BestTest: 0.0793\n",
      "Epoch : 161/1200, train_loss: 0.1189, test_loss: 0.1051, BestTest: 0.0793\n",
      "Epoch : 162/1200, train_loss: 0.1176, test_loss: 0.1510, BestTest: 0.0793\n",
      "Epoch : 163/1200, train_loss: 0.1205, test_loss: 0.1059, BestTest: 0.0793\n",
      "Epoch : 164/1200, train_loss: 0.1199, test_loss: 0.1010, BestTest: 0.0793\n",
      "Epoch : 165/1200, train_loss: 0.1147, test_loss: 0.1156, BestTest: 0.0793\n",
      "Epoch : 166/1200, train_loss: 0.1248, test_loss: 0.1132, BestTest: 0.0793\n",
      "Epoch : 167/1200, train_loss: 0.1151, test_loss: 0.1109, BestTest: 0.0793\n",
      "Epoch : 168/1200, train_loss: 0.1252, test_loss: 0.0942, BestTest: 0.0793\n",
      "Epoch : 169/1200, train_loss: 0.1178, test_loss: 0.0954, BestTest: 0.0793\n",
      "Epoch : 170/1200, train_loss: 0.1127, test_loss: 0.1135, BestTest: 0.0793\n",
      "Epoch : 171/1200, train_loss: 0.1165, test_loss: 0.1103, BestTest: 0.0793\n",
      "Epoch : 172/1200, train_loss: 0.1154, test_loss: 0.1107, BestTest: 0.0793\n",
      "Epoch : 173/1200, train_loss: 0.1212, test_loss: 0.1068, BestTest: 0.0793\n",
      "Epoch : 174/1200, train_loss: 0.1180, test_loss: 0.0864, BestTest: 0.0793\n",
      "Epoch : 175/1200, train_loss: 0.1099, test_loss: 0.1136, BestTest: 0.0793\n",
      "Epoch : 176/1200, train_loss: 0.1193, test_loss: 0.1219, BestTest: 0.0793\n",
      "Epoch : 177/1200, train_loss: 0.1139, test_loss: 0.0909, BestTest: 0.0793\n",
      "Epoch : 178/1200, train_loss: 0.1112, test_loss: 0.1035, BestTest: 0.0793\n",
      "Epoch : 179/1200, train_loss: 0.1126, test_loss: 0.0918, BestTest: 0.0793\n",
      "Epoch : 180/1200, train_loss: 0.1100, test_loss: 0.1188, BestTest: 0.0793\n",
      "Epoch : 181/1200, train_loss: 0.1117, test_loss: 0.1070, BestTest: 0.0793\n",
      "Epoch : 182/1200, train_loss: 0.1165, test_loss: 0.1135, BestTest: 0.0793\n",
      "Epoch : 183/1200, train_loss: 0.1168, test_loss: 0.1189, BestTest: 0.0793\n",
      "Epoch : 184/1200, train_loss: 0.1187, test_loss: 0.0899, BestTest: 0.0793\n",
      "Epoch : 185/1200, train_loss: 0.1121, test_loss: 0.0854, BestTest: 0.0793\n",
      "Epoch : 186/1200, train_loss: 0.1141, test_loss: 0.0928, BestTest: 0.0793\n",
      "Epoch : 187/1200, train_loss: 0.1174, test_loss: 0.0957, BestTest: 0.0793\n",
      "Epoch : 188/1200, train_loss: 0.1110, test_loss: 0.1048, BestTest: 0.0793\n",
      "Epoch : 189/1200, train_loss: 0.1175, test_loss: 0.0926, BestTest: 0.0793\n",
      "Epoch : 190/1200, train_loss: 0.1166, test_loss: 0.1014, BestTest: 0.0793\n",
      "Epoch : 191/1200, train_loss: 0.1060, test_loss: 0.1423, BestTest: 0.0793\n",
      "Epoch : 192/1200, train_loss: 0.1107, test_loss: 0.0959, BestTest: 0.0793\n",
      "Epoch : 193/1200, train_loss: 0.1153, test_loss: 0.0853, BestTest: 0.0793\n",
      "Epoch : 194/1200, train_loss: 0.1125, test_loss: 0.0987, BestTest: 0.0793\n",
      "Epoch : 195/1200, train_loss: 0.1115, test_loss: 0.0828, BestTest: 0.0793\n",
      "Epoch : 196/1200, train_loss: 0.1146, test_loss: 0.1106, BestTest: 0.0793\n",
      "Epoch : 197/1200, train_loss: 0.1083, test_loss: 0.0792, BestTest: 0.0792\n",
      "Epoch : 198/1200, train_loss: 0.1136, test_loss: 0.1047, BestTest: 0.0792\n",
      "Epoch : 199/1200, train_loss: 0.1099, test_loss: 0.1107, BestTest: 0.0792\n",
      "Epoch : 200/1200, train_loss: 0.0787, test_loss: 0.0645, BestTest: 0.0645\n",
      "Epoch : 201/1200, train_loss: 0.0830, test_loss: 0.0580, BestTest: 0.0580\n",
      "Epoch : 202/1200, train_loss: 0.0789, test_loss: 0.0878, BestTest: 0.0580\n",
      "Epoch : 203/1200, train_loss: 0.0792, test_loss: 0.0996, BestTest: 0.0580\n",
      "Epoch : 204/1200, train_loss: 0.0814, test_loss: 0.1031, BestTest: 0.0580\n",
      "Epoch : 205/1200, train_loss: 0.0753, test_loss: 0.0698, BestTest: 0.0580\n",
      "Epoch : 206/1200, train_loss: 0.0811, test_loss: 0.0690, BestTest: 0.0580\n",
      "Epoch : 207/1200, train_loss: 0.0807, test_loss: 0.0811, BestTest: 0.0580\n",
      "Epoch : 208/1200, train_loss: 0.0793, test_loss: 0.0648, BestTest: 0.0580\n",
      "Epoch : 209/1200, train_loss: 0.0819, test_loss: 0.0607, BestTest: 0.0580\n",
      "Epoch : 210/1200, train_loss: 0.0752, test_loss: 0.0607, BestTest: 0.0580\n",
      "Epoch : 211/1200, train_loss: 0.0713, test_loss: 0.0693, BestTest: 0.0580\n",
      "Epoch : 212/1200, train_loss: 0.0744, test_loss: 0.0920, BestTest: 0.0580\n",
      "Epoch : 213/1200, train_loss: 0.0787, test_loss: 0.0606, BestTest: 0.0580\n",
      "Epoch : 214/1200, train_loss: 0.0745, test_loss: 0.0524, BestTest: 0.0524\n",
      "Epoch : 215/1200, train_loss: 0.0725, test_loss: 0.0544, BestTest: 0.0524\n",
      "Epoch : 216/1200, train_loss: 0.0737, test_loss: 0.0620, BestTest: 0.0524\n",
      "Epoch : 217/1200, train_loss: 0.0764, test_loss: 0.0585, BestTest: 0.0524\n",
      "Epoch : 218/1200, train_loss: 0.0739, test_loss: 0.0747, BestTest: 0.0524\n",
      "Epoch : 219/1200, train_loss: 0.0710, test_loss: 0.0676, BestTest: 0.0524\n",
      "Epoch : 220/1200, train_loss: 0.0762, test_loss: 0.0712, BestTest: 0.0524\n",
      "Epoch : 221/1200, train_loss: 0.0742, test_loss: 0.0877, BestTest: 0.0524\n",
      "Epoch : 222/1200, train_loss: 0.0750, test_loss: 0.0737, BestTest: 0.0524\n",
      "Epoch : 223/1200, train_loss: 0.0750, test_loss: 0.0486, BestTest: 0.0486\n",
      "Epoch : 224/1200, train_loss: 0.0802, test_loss: 0.0489, BestTest: 0.0486\n",
      "Epoch : 225/1200, train_loss: 0.0748, test_loss: 0.0583, BestTest: 0.0486\n",
      "Epoch : 226/1200, train_loss: 0.0761, test_loss: 0.0795, BestTest: 0.0486\n",
      "Epoch : 227/1200, train_loss: 0.0697, test_loss: 0.0586, BestTest: 0.0486\n",
      "Epoch : 228/1200, train_loss: 0.0732, test_loss: 0.0748, BestTest: 0.0486\n",
      "Epoch : 229/1200, train_loss: 0.0732, test_loss: 0.0613, BestTest: 0.0486\n",
      "Epoch : 230/1200, train_loss: 0.0717, test_loss: 0.0920, BestTest: 0.0486\n",
      "Epoch : 231/1200, train_loss: 0.0718, test_loss: 0.0574, BestTest: 0.0486\n",
      "Epoch : 232/1200, train_loss: 0.0710, test_loss: 0.0667, BestTest: 0.0486\n",
      "Epoch : 233/1200, train_loss: 0.0708, test_loss: 0.0532, BestTest: 0.0486\n",
      "Epoch : 234/1200, train_loss: 0.0682, test_loss: 0.0501, BestTest: 0.0486\n",
      "Epoch : 235/1200, train_loss: 0.0723, test_loss: 0.0486, BestTest: 0.0486\n",
      "Epoch : 236/1200, train_loss: 0.0715, test_loss: 0.0914, BestTest: 0.0486\n",
      "Epoch : 237/1200, train_loss: 0.0720, test_loss: 0.0611, BestTest: 0.0486\n",
      "Epoch : 238/1200, train_loss: 0.0697, test_loss: 0.0477, BestTest: 0.0477\n",
      "Epoch : 239/1200, train_loss: 0.0690, test_loss: 0.0711, BestTest: 0.0477\n",
      "Epoch : 240/1200, train_loss: 0.0755, test_loss: 0.1162, BestTest: 0.0477\n",
      "Epoch : 241/1200, train_loss: 0.0727, test_loss: 0.0892, BestTest: 0.0477\n",
      "Epoch : 242/1200, train_loss: 0.0708, test_loss: 0.0615, BestTest: 0.0477\n",
      "Epoch : 243/1200, train_loss: 0.0712, test_loss: 0.0487, BestTest: 0.0477\n",
      "Epoch : 244/1200, train_loss: 0.0684, test_loss: 0.0597, BestTest: 0.0477\n",
      "Epoch : 245/1200, train_loss: 0.0663, test_loss: 0.0604, BestTest: 0.0477\n",
      "Epoch : 246/1200, train_loss: 0.0707, test_loss: 0.0596, BestTest: 0.0477\n",
      "Epoch : 247/1200, train_loss: 0.0701, test_loss: 0.0646, BestTest: 0.0477\n",
      "Epoch : 248/1200, train_loss: 0.0736, test_loss: 0.0526, BestTest: 0.0477\n",
      "Epoch : 249/1200, train_loss: 0.0699, test_loss: 0.0532, BestTest: 0.0477\n",
      "Epoch : 250/1200, train_loss: 0.0733, test_loss: 0.0727, BestTest: 0.0477\n",
      "Epoch : 251/1200, train_loss: 0.0689, test_loss: 0.0591, BestTest: 0.0477\n",
      "Epoch : 252/1200, train_loss: 0.0717, test_loss: 0.0600, BestTest: 0.0477\n",
      "Epoch : 253/1200, train_loss: 0.0677, test_loss: 0.0564, BestTest: 0.0477\n",
      "Epoch : 254/1200, train_loss: 0.0734, test_loss: 0.0832, BestTest: 0.0477\n",
      "Epoch : 255/1200, train_loss: 0.0693, test_loss: 0.0546, BestTest: 0.0477\n",
      "Epoch : 256/1200, train_loss: 0.0679, test_loss: 0.0668, BestTest: 0.0477\n",
      "Epoch : 257/1200, train_loss: 0.0748, test_loss: 0.0578, BestTest: 0.0477\n",
      "Epoch : 258/1200, train_loss: 0.0702, test_loss: 0.0550, BestTest: 0.0477\n",
      "Epoch : 259/1200, train_loss: 0.0700, test_loss: 0.0801, BestTest: 0.0477\n",
      "Epoch : 260/1200, train_loss: 0.0705, test_loss: 0.0600, BestTest: 0.0477\n",
      "Epoch : 261/1200, train_loss: 0.0701, test_loss: 0.0551, BestTest: 0.0477\n",
      "Epoch : 262/1200, train_loss: 0.0695, test_loss: 0.0633, BestTest: 0.0477\n",
      "Epoch : 263/1200, train_loss: 0.0697, test_loss: 0.0445, BestTest: 0.0445\n",
      "Epoch : 264/1200, train_loss: 0.0652, test_loss: 0.0598, BestTest: 0.0445\n",
      "Epoch : 265/1200, train_loss: 0.0717, test_loss: 0.0517, BestTest: 0.0445\n",
      "Epoch : 266/1200, train_loss: 0.0721, test_loss: 0.0650, BestTest: 0.0445\n",
      "Epoch : 267/1200, train_loss: 0.0707, test_loss: 0.0748, BestTest: 0.0445\n",
      "Epoch : 268/1200, train_loss: 0.0691, test_loss: 0.0649, BestTest: 0.0445\n",
      "Epoch : 269/1200, train_loss: 0.0665, test_loss: 0.0842, BestTest: 0.0445\n",
      "Epoch : 270/1200, train_loss: 0.0650, test_loss: 0.0603, BestTest: 0.0445\n",
      "Epoch : 271/1200, train_loss: 0.0690, test_loss: 0.0495, BestTest: 0.0445\n",
      "Epoch : 272/1200, train_loss: 0.0708, test_loss: 0.0493, BestTest: 0.0445\n",
      "Epoch : 273/1200, train_loss: 0.0678, test_loss: 0.0633, BestTest: 0.0445\n",
      "Epoch : 274/1200, train_loss: 0.0726, test_loss: 0.0679, BestTest: 0.0445\n",
      "Epoch : 275/1200, train_loss: 0.0687, test_loss: 0.0757, BestTest: 0.0445\n",
      "Epoch : 276/1200, train_loss: 0.0676, test_loss: 0.0644, BestTest: 0.0445\n",
      "Epoch : 277/1200, train_loss: 0.0661, test_loss: 0.0502, BestTest: 0.0445\n",
      "Epoch : 278/1200, train_loss: 0.0653, test_loss: 0.0921, BestTest: 0.0445\n",
      "Epoch : 279/1200, train_loss: 0.0676, test_loss: 0.0803, BestTest: 0.0445\n",
      "Epoch : 280/1200, train_loss: 0.0696, test_loss: 0.0722, BestTest: 0.0445\n",
      "Epoch : 281/1200, train_loss: 0.0678, test_loss: 0.0509, BestTest: 0.0445\n",
      "Epoch : 282/1200, train_loss: 0.0651, test_loss: 0.0501, BestTest: 0.0445\n",
      "Epoch : 283/1200, train_loss: 0.0672, test_loss: 0.0655, BestTest: 0.0445\n",
      "Epoch : 284/1200, train_loss: 0.0700, test_loss: 0.0605, BestTest: 0.0445\n",
      "Epoch : 285/1200, train_loss: 0.0677, test_loss: 0.0692, BestTest: 0.0445\n",
      "Epoch : 286/1200, train_loss: 0.0643, test_loss: 0.0883, BestTest: 0.0445\n",
      "Epoch : 287/1200, train_loss: 0.0675, test_loss: 0.0554, BestTest: 0.0445\n",
      "Epoch : 288/1200, train_loss: 0.0637, test_loss: 0.0483, BestTest: 0.0445\n",
      "Epoch : 289/1200, train_loss: 0.0692, test_loss: 0.0617, BestTest: 0.0445\n",
      "Epoch : 290/1200, train_loss: 0.0666, test_loss: 0.0733, BestTest: 0.0445\n",
      "Epoch : 291/1200, train_loss: 0.0632, test_loss: 0.0613, BestTest: 0.0445\n",
      "Epoch : 292/1200, train_loss: 0.0707, test_loss: 0.0920, BestTest: 0.0445\n",
      "Epoch : 293/1200, train_loss: 0.0658, test_loss: 0.0510, BestTest: 0.0445\n",
      "Epoch : 294/1200, train_loss: 0.0666, test_loss: 0.0656, BestTest: 0.0445\n",
      "Epoch : 295/1200, train_loss: 0.0662, test_loss: 0.0650, BestTest: 0.0445\n",
      "Epoch : 296/1200, train_loss: 0.0644, test_loss: 0.0513, BestTest: 0.0445\n",
      "Epoch : 297/1200, train_loss: 0.0673, test_loss: 0.0496, BestTest: 0.0445\n",
      "Epoch : 298/1200, train_loss: 0.0637, test_loss: 0.0672, BestTest: 0.0445\n",
      "Epoch : 299/1200, train_loss: 0.0665, test_loss: 0.0564, BestTest: 0.0445\n",
      "Epoch : 300/1200, train_loss: 0.0512, test_loss: 0.0569, BestTest: 0.0445\n",
      "Epoch : 301/1200, train_loss: 0.0499, test_loss: 0.0472, BestTest: 0.0445\n",
      "Epoch : 302/1200, train_loss: 0.0500, test_loss: 0.0541, BestTest: 0.0445\n",
      "Epoch : 303/1200, train_loss: 0.0482, test_loss: 0.0339, BestTest: 0.0339\n",
      "Epoch : 304/1200, train_loss: 0.0514, test_loss: 0.0601, BestTest: 0.0339\n",
      "Epoch : 305/1200, train_loss: 0.0503, test_loss: 0.0643, BestTest: 0.0339\n",
      "Epoch : 306/1200, train_loss: 0.0490, test_loss: 0.0288, BestTest: 0.0288\n",
      "Epoch : 307/1200, train_loss: 0.0508, test_loss: 0.0369, BestTest: 0.0288\n",
      "Epoch : 308/1200, train_loss: 0.0527, test_loss: 0.0362, BestTest: 0.0288\n",
      "Epoch : 309/1200, train_loss: 0.0471, test_loss: 0.0439, BestTest: 0.0288\n",
      "Epoch : 310/1200, train_loss: 0.0476, test_loss: 0.0299, BestTest: 0.0288\n",
      "Epoch : 311/1200, train_loss: 0.0500, test_loss: 0.0446, BestTest: 0.0288\n",
      "Epoch : 312/1200, train_loss: 0.0486, test_loss: 0.0318, BestTest: 0.0288\n",
      "Epoch : 313/1200, train_loss: 0.0489, test_loss: 0.0388, BestTest: 0.0288\n",
      "Epoch : 314/1200, train_loss: 0.0498, test_loss: 0.0469, BestTest: 0.0288\n",
      "Epoch : 315/1200, train_loss: 0.0486, test_loss: 0.0358, BestTest: 0.0288\n",
      "Epoch : 316/1200, train_loss: 0.0482, test_loss: 0.0394, BestTest: 0.0288\n",
      "Epoch : 317/1200, train_loss: 0.0476, test_loss: 0.0437, BestTest: 0.0288\n",
      "Epoch : 318/1200, train_loss: 0.0518, test_loss: 0.0387, BestTest: 0.0288\n",
      "Epoch : 319/1200, train_loss: 0.0465, test_loss: 0.0380, BestTest: 0.0288\n",
      "Epoch : 320/1200, train_loss: 0.0485, test_loss: 0.0358, BestTest: 0.0288\n",
      "Epoch : 321/1200, train_loss: 0.0485, test_loss: 0.0310, BestTest: 0.0288\n",
      "Epoch : 322/1200, train_loss: 0.0469, test_loss: 0.0384, BestTest: 0.0288\n",
      "Epoch : 323/1200, train_loss: 0.0476, test_loss: 0.0681, BestTest: 0.0288\n",
      "Epoch : 324/1200, train_loss: 0.0484, test_loss: 0.0372, BestTest: 0.0288\n",
      "Epoch : 325/1200, train_loss: 0.0489, test_loss: 0.0408, BestTest: 0.0288\n",
      "Epoch : 326/1200, train_loss: 0.0486, test_loss: 0.0399, BestTest: 0.0288\n",
      "Epoch : 327/1200, train_loss: 0.0469, test_loss: 0.0298, BestTest: 0.0288\n",
      "Epoch : 328/1200, train_loss: 0.0469, test_loss: 0.0306, BestTest: 0.0288\n",
      "Epoch : 329/1200, train_loss: 0.0479, test_loss: 0.0461, BestTest: 0.0288\n",
      "Epoch : 330/1200, train_loss: 0.0481, test_loss: 0.0409, BestTest: 0.0288\n",
      "Epoch : 331/1200, train_loss: 0.0481, test_loss: 0.0430, BestTest: 0.0288\n",
      "Epoch : 332/1200, train_loss: 0.0464, test_loss: 0.0330, BestTest: 0.0288\n",
      "Epoch : 333/1200, train_loss: 0.0482, test_loss: 0.0343, BestTest: 0.0288\n",
      "Epoch : 334/1200, train_loss: 0.0468, test_loss: 0.0447, BestTest: 0.0288\n",
      "Epoch : 335/1200, train_loss: 0.0455, test_loss: 0.0375, BestTest: 0.0288\n",
      "Epoch : 336/1200, train_loss: 0.0452, test_loss: 0.0415, BestTest: 0.0288\n",
      "Epoch : 337/1200, train_loss: 0.0492, test_loss: 0.0547, BestTest: 0.0288\n",
      "Epoch : 338/1200, train_loss: 0.0462, test_loss: 0.0426, BestTest: 0.0288\n",
      "Epoch : 339/1200, train_loss: 0.0488, test_loss: 0.0369, BestTest: 0.0288\n",
      "Epoch : 340/1200, train_loss: 0.0459, test_loss: 0.0375, BestTest: 0.0288\n",
      "Epoch : 341/1200, train_loss: 0.0473, test_loss: 0.0473, BestTest: 0.0288\n",
      "Epoch : 342/1200, train_loss: 0.0488, test_loss: 0.0411, BestTest: 0.0288\n",
      "Epoch : 343/1200, train_loss: 0.0452, test_loss: 0.0330, BestTest: 0.0288\n",
      "Epoch : 344/1200, train_loss: 0.0462, test_loss: 0.0751, BestTest: 0.0288\n",
      "Epoch : 345/1200, train_loss: 0.0480, test_loss: 0.0310, BestTest: 0.0288\n",
      "Epoch : 346/1200, train_loss: 0.0464, test_loss: 0.0430, BestTest: 0.0288\n",
      "Epoch : 347/1200, train_loss: 0.0460, test_loss: 0.0487, BestTest: 0.0288\n",
      "Epoch : 348/1200, train_loss: 0.0453, test_loss: 0.0311, BestTest: 0.0288\n",
      "Epoch : 349/1200, train_loss: 0.0465, test_loss: 0.0388, BestTest: 0.0288\n",
      "Epoch : 350/1200, train_loss: 0.0471, test_loss: 0.0372, BestTest: 0.0288\n",
      "Epoch : 351/1200, train_loss: 0.0451, test_loss: 0.0503, BestTest: 0.0288\n",
      "Epoch : 352/1200, train_loss: 0.0510, test_loss: 0.0491, BestTest: 0.0288\n",
      "Epoch : 353/1200, train_loss: 0.0461, test_loss: 0.0515, BestTest: 0.0288\n",
      "Epoch : 354/1200, train_loss: 0.0452, test_loss: 0.0505, BestTest: 0.0288\n",
      "Epoch : 355/1200, train_loss: 0.0464, test_loss: 0.0347, BestTest: 0.0288\n",
      "Epoch : 356/1200, train_loss: 0.0450, test_loss: 0.0461, BestTest: 0.0288\n",
      "Epoch : 357/1200, train_loss: 0.0447, test_loss: 0.0598, BestTest: 0.0288\n",
      "Epoch : 358/1200, train_loss: 0.0477, test_loss: 0.0359, BestTest: 0.0288\n",
      "Epoch : 359/1200, train_loss: 0.0447, test_loss: 0.0416, BestTest: 0.0288\n",
      "Epoch : 360/1200, train_loss: 0.0445, test_loss: 0.0352, BestTest: 0.0288\n",
      "Epoch : 361/1200, train_loss: 0.0459, test_loss: 0.0718, BestTest: 0.0288\n",
      "Epoch : 362/1200, train_loss: 0.0480, test_loss: 0.0427, BestTest: 0.0288\n",
      "Epoch : 363/1200, train_loss: 0.0480, test_loss: 0.0331, BestTest: 0.0288\n",
      "Epoch : 364/1200, train_loss: 0.0460, test_loss: 0.0391, BestTest: 0.0288\n",
      "Epoch : 365/1200, train_loss: 0.0463, test_loss: 0.0587, BestTest: 0.0288\n",
      "Epoch : 366/1200, train_loss: 0.0472, test_loss: 0.0325, BestTest: 0.0288\n",
      "Epoch : 367/1200, train_loss: 0.0467, test_loss: 0.0441, BestTest: 0.0288\n",
      "Epoch : 368/1200, train_loss: 0.0456, test_loss: 0.0570, BestTest: 0.0288\n",
      "Epoch : 369/1200, train_loss: 0.0466, test_loss: 0.0318, BestTest: 0.0288\n",
      "Epoch : 370/1200, train_loss: 0.0463, test_loss: 0.0382, BestTest: 0.0288\n",
      "Epoch : 371/1200, train_loss: 0.0454, test_loss: 0.0435, BestTest: 0.0288\n",
      "Epoch : 372/1200, train_loss: 0.0457, test_loss: 0.0486, BestTest: 0.0288\n",
      "Epoch : 373/1200, train_loss: 0.0474, test_loss: 0.0310, BestTest: 0.0288\n",
      "Epoch : 374/1200, train_loss: 0.0431, test_loss: 0.0524, BestTest: 0.0288\n",
      "Epoch : 375/1200, train_loss: 0.0480, test_loss: 0.0418, BestTest: 0.0288\n",
      "Epoch : 376/1200, train_loss: 0.0467, test_loss: 0.0388, BestTest: 0.0288\n",
      "Epoch : 377/1200, train_loss: 0.0452, test_loss: 0.0344, BestTest: 0.0288\n",
      "Epoch : 378/1200, train_loss: 0.0466, test_loss: 0.0382, BestTest: 0.0288\n",
      "Epoch : 379/1200, train_loss: 0.0465, test_loss: 0.0550, BestTest: 0.0288\n",
      "Epoch : 380/1200, train_loss: 0.0455, test_loss: 0.0298, BestTest: 0.0288\n",
      "Epoch : 381/1200, train_loss: 0.0457, test_loss: 0.0449, BestTest: 0.0288\n",
      "Epoch : 382/1200, train_loss: 0.0471, test_loss: 0.0279, BestTest: 0.0279\n",
      "Epoch : 383/1200, train_loss: 0.0484, test_loss: 0.0478, BestTest: 0.0279\n",
      "Epoch : 384/1200, train_loss: 0.0441, test_loss: 0.0367, BestTest: 0.0279\n",
      "Epoch : 385/1200, train_loss: 0.0445, test_loss: 0.0311, BestTest: 0.0279\n",
      "Epoch : 386/1200, train_loss: 0.0447, test_loss: 0.0355, BestTest: 0.0279\n",
      "Epoch : 387/1200, train_loss: 0.0467, test_loss: 0.0436, BestTest: 0.0279\n",
      "Epoch : 388/1200, train_loss: 0.0458, test_loss: 0.0336, BestTest: 0.0279\n",
      "Epoch : 389/1200, train_loss: 0.0445, test_loss: 0.0605, BestTest: 0.0279\n",
      "Epoch : 390/1200, train_loss: 0.0445, test_loss: 0.0430, BestTest: 0.0279\n",
      "Epoch : 391/1200, train_loss: 0.0453, test_loss: 0.0343, BestTest: 0.0279\n",
      "Epoch : 392/1200, train_loss: 0.0468, test_loss: 0.0585, BestTest: 0.0279\n",
      "Epoch : 393/1200, train_loss: 0.0448, test_loss: 0.0590, BestTest: 0.0279\n",
      "Epoch : 394/1200, train_loss: 0.0442, test_loss: 0.0324, BestTest: 0.0279\n",
      "Epoch : 395/1200, train_loss: 0.0444, test_loss: 0.0347, BestTest: 0.0279\n",
      "Epoch : 396/1200, train_loss: 0.0447, test_loss: 0.0398, BestTest: 0.0279\n",
      "Epoch : 397/1200, train_loss: 0.0434, test_loss: 0.0469, BestTest: 0.0279\n",
      "Epoch : 398/1200, train_loss: 0.0466, test_loss: 0.0430, BestTest: 0.0279\n",
      "Epoch : 399/1200, train_loss: 0.0424, test_loss: 0.0293, BestTest: 0.0279\n",
      "Epoch : 400/1200, train_loss: 0.0369, test_loss: 0.0301, BestTest: 0.0279\n",
      "Epoch : 401/1200, train_loss: 0.0359, test_loss: 0.0336, BestTest: 0.0279\n",
      "Epoch : 402/1200, train_loss: 0.0361, test_loss: 0.0367, BestTest: 0.0279\n",
      "Epoch : 403/1200, train_loss: 0.0349, test_loss: 0.0414, BestTest: 0.0279\n",
      "Epoch : 404/1200, train_loss: 0.0372, test_loss: 0.0262, BestTest: 0.0262\n",
      "Epoch : 405/1200, train_loss: 0.0351, test_loss: 0.0255, BestTest: 0.0255\n",
      "Epoch : 406/1200, train_loss: 0.0368, test_loss: 0.0319, BestTest: 0.0255\n",
      "Epoch : 407/1200, train_loss: 0.0350, test_loss: 0.0354, BestTest: 0.0255\n",
      "Epoch : 408/1200, train_loss: 0.0354, test_loss: 0.0321, BestTest: 0.0255\n",
      "Epoch : 409/1200, train_loss: 0.0361, test_loss: 0.0229, BestTest: 0.0229\n",
      "Epoch : 410/1200, train_loss: 0.0370, test_loss: 0.0296, BestTest: 0.0229\n",
      "Epoch : 411/1200, train_loss: 0.0365, test_loss: 0.0287, BestTest: 0.0229\n",
      "Epoch : 412/1200, train_loss: 0.0369, test_loss: 0.0410, BestTest: 0.0229\n",
      "Epoch : 413/1200, train_loss: 0.0350, test_loss: 0.0391, BestTest: 0.0229\n",
      "Epoch : 414/1200, train_loss: 0.0354, test_loss: 0.0226, BestTest: 0.0226\n",
      "Epoch : 415/1200, train_loss: 0.0360, test_loss: 0.0242, BestTest: 0.0226\n",
      "Epoch : 416/1200, train_loss: 0.0363, test_loss: 0.0371, BestTest: 0.0226\n",
      "Epoch : 417/1200, train_loss: 0.0350, test_loss: 0.0314, BestTest: 0.0226\n",
      "Epoch : 418/1200, train_loss: 0.0349, test_loss: 0.0279, BestTest: 0.0226\n",
      "Epoch : 419/1200, train_loss: 0.0363, test_loss: 0.0325, BestTest: 0.0226\n",
      "Epoch : 420/1200, train_loss: 0.0355, test_loss: 0.0395, BestTest: 0.0226\n",
      "Epoch : 421/1200, train_loss: 0.0365, test_loss: 0.0324, BestTest: 0.0226\n",
      "Epoch : 422/1200, train_loss: 0.0349, test_loss: 0.0219, BestTest: 0.0219\n",
      "Epoch : 423/1200, train_loss: 0.0352, test_loss: 0.0317, BestTest: 0.0219\n",
      "Epoch : 424/1200, train_loss: 0.0344, test_loss: 0.0285, BestTest: 0.0219\n",
      "Epoch : 425/1200, train_loss: 0.0361, test_loss: 0.0356, BestTest: 0.0219\n",
      "Epoch : 426/1200, train_loss: 0.0375, test_loss: 0.0310, BestTest: 0.0219\n",
      "Epoch : 427/1200, train_loss: 0.0346, test_loss: 0.0317, BestTest: 0.0219\n",
      "Epoch : 428/1200, train_loss: 0.0345, test_loss: 0.0221, BestTest: 0.0219\n",
      "Epoch : 429/1200, train_loss: 0.0368, test_loss: 0.0337, BestTest: 0.0219\n",
      "Epoch : 430/1200, train_loss: 0.0364, test_loss: 0.0266, BestTest: 0.0219\n",
      "Epoch : 431/1200, train_loss: 0.0368, test_loss: 0.0242, BestTest: 0.0219\n",
      "Epoch : 432/1200, train_loss: 0.0359, test_loss: 0.0334, BestTest: 0.0219\n",
      "Epoch : 433/1200, train_loss: 0.0344, test_loss: 0.0357, BestTest: 0.0219\n",
      "Epoch : 434/1200, train_loss: 0.0359, test_loss: 0.0267, BestTest: 0.0219\n",
      "Epoch : 435/1200, train_loss: 0.0352, test_loss: 0.0275, BestTest: 0.0219\n",
      "Epoch : 436/1200, train_loss: 0.0345, test_loss: 0.0307, BestTest: 0.0219\n",
      "Epoch : 437/1200, train_loss: 0.0370, test_loss: 0.0378, BestTest: 0.0219\n",
      "Epoch : 438/1200, train_loss: 0.0346, test_loss: 0.0269, BestTest: 0.0219\n",
      "Epoch : 439/1200, train_loss: 0.0358, test_loss: 0.0235, BestTest: 0.0219\n",
      "Epoch : 440/1200, train_loss: 0.0362, test_loss: 0.0256, BestTest: 0.0219\n",
      "Epoch : 441/1200, train_loss: 0.0354, test_loss: 0.0302, BestTest: 0.0219\n",
      "Epoch : 442/1200, train_loss: 0.0352, test_loss: 0.0347, BestTest: 0.0219\n",
      "Epoch : 443/1200, train_loss: 0.0368, test_loss: 0.0297, BestTest: 0.0219\n",
      "Epoch : 444/1200, train_loss: 0.0346, test_loss: 0.0240, BestTest: 0.0219\n",
      "Epoch : 445/1200, train_loss: 0.0356, test_loss: 0.0191, BestTest: 0.0191\n",
      "Epoch : 446/1200, train_loss: 0.0351, test_loss: 0.0285, BestTest: 0.0191\n",
      "Epoch : 447/1200, train_loss: 0.0364, test_loss: 0.0257, BestTest: 0.0191\n",
      "Epoch : 448/1200, train_loss: 0.0353, test_loss: 0.0237, BestTest: 0.0191\n",
      "Epoch : 449/1200, train_loss: 0.0348, test_loss: 0.0270, BestTest: 0.0191\n",
      "Epoch : 450/1200, train_loss: 0.0345, test_loss: 0.0334, BestTest: 0.0191\n",
      "Epoch : 451/1200, train_loss: 0.0360, test_loss: 0.0277, BestTest: 0.0191\n",
      "Epoch : 452/1200, train_loss: 0.0355, test_loss: 0.0352, BestTest: 0.0191\n",
      "Epoch : 453/1200, train_loss: 0.0335, test_loss: 0.0333, BestTest: 0.0191\n",
      "Epoch : 454/1200, train_loss: 0.0349, test_loss: 0.0257, BestTest: 0.0191\n",
      "Epoch : 455/1200, train_loss: 0.0349, test_loss: 0.0261, BestTest: 0.0191\n",
      "Epoch : 456/1200, train_loss: 0.0350, test_loss: 0.0275, BestTest: 0.0191\n",
      "Epoch : 457/1200, train_loss: 0.0351, test_loss: 0.0339, BestTest: 0.0191\n",
      "Epoch : 458/1200, train_loss: 0.0347, test_loss: 0.0304, BestTest: 0.0191\n",
      "Epoch : 459/1200, train_loss: 0.0337, test_loss: 0.0250, BestTest: 0.0191\n",
      "Epoch : 460/1200, train_loss: 0.0359, test_loss: 0.0360, BestTest: 0.0191\n",
      "Epoch : 461/1200, train_loss: 0.0344, test_loss: 0.0380, BestTest: 0.0191\n",
      "Epoch : 462/1200, train_loss: 0.0343, test_loss: 0.0239, BestTest: 0.0191\n",
      "Epoch : 463/1200, train_loss: 0.0349, test_loss: 0.0318, BestTest: 0.0191\n",
      "Epoch : 464/1200, train_loss: 0.0361, test_loss: 0.0372, BestTest: 0.0191\n",
      "Epoch : 465/1200, train_loss: 0.0348, test_loss: 0.0316, BestTest: 0.0191\n",
      "Epoch : 466/1200, train_loss: 0.0350, test_loss: 0.0312, BestTest: 0.0191\n",
      "Epoch : 467/1200, train_loss: 0.0349, test_loss: 0.0465, BestTest: 0.0191\n",
      "Epoch : 468/1200, train_loss: 0.0350, test_loss: 0.0325, BestTest: 0.0191\n",
      "Epoch : 469/1200, train_loss: 0.0361, test_loss: 0.0403, BestTest: 0.0191\n",
      "Epoch : 470/1200, train_loss: 0.0345, test_loss: 0.0254, BestTest: 0.0191\n",
      "Epoch : 471/1200, train_loss: 0.0332, test_loss: 0.0262, BestTest: 0.0191\n",
      "Epoch : 472/1200, train_loss: 0.0335, test_loss: 0.0236, BestTest: 0.0191\n",
      "Epoch : 473/1200, train_loss: 0.0341, test_loss: 0.0343, BestTest: 0.0191\n",
      "Epoch : 474/1200, train_loss: 0.0352, test_loss: 0.0205, BestTest: 0.0191\n",
      "Epoch : 475/1200, train_loss: 0.0342, test_loss: 0.0407, BestTest: 0.0191\n",
      "Epoch : 476/1200, train_loss: 0.0356, test_loss: 0.0226, BestTest: 0.0191\n",
      "Epoch : 477/1200, train_loss: 0.0334, test_loss: 0.0256, BestTest: 0.0191\n",
      "Epoch : 478/1200, train_loss: 0.0336, test_loss: 0.0262, BestTest: 0.0191\n",
      "Epoch : 479/1200, train_loss: 0.0342, test_loss: 0.0411, BestTest: 0.0191\n",
      "Epoch : 480/1200, train_loss: 0.0342, test_loss: 0.0381, BestTest: 0.0191\n",
      "Epoch : 481/1200, train_loss: 0.0342, test_loss: 0.0347, BestTest: 0.0191\n",
      "Epoch : 482/1200, train_loss: 0.0353, test_loss: 0.0387, BestTest: 0.0191\n",
      "Epoch : 483/1200, train_loss: 0.0336, test_loss: 0.0186, BestTest: 0.0186\n",
      "Epoch : 484/1200, train_loss: 0.0339, test_loss: 0.0435, BestTest: 0.0186\n",
      "Epoch : 485/1200, train_loss: 0.0343, test_loss: 0.0369, BestTest: 0.0186\n",
      "Epoch : 486/1200, train_loss: 0.0337, test_loss: 0.0244, BestTest: 0.0186\n",
      "Epoch : 487/1200, train_loss: 0.0332, test_loss: 0.0338, BestTest: 0.0186\n",
      "Epoch : 488/1200, train_loss: 0.0354, test_loss: 0.0266, BestTest: 0.0186\n",
      "Epoch : 489/1200, train_loss: 0.0358, test_loss: 0.0380, BestTest: 0.0186\n",
      "Epoch : 490/1200, train_loss: 0.0351, test_loss: 0.0311, BestTest: 0.0186\n",
      "Epoch : 491/1200, train_loss: 0.0349, test_loss: 0.0368, BestTest: 0.0186\n",
      "Epoch : 492/1200, train_loss: 0.0341, test_loss: 0.0362, BestTest: 0.0186\n",
      "Epoch : 493/1200, train_loss: 0.0343, test_loss: 0.0288, BestTest: 0.0186\n",
      "Epoch : 494/1200, train_loss: 0.0334, test_loss: 0.0323, BestTest: 0.0186\n",
      "Epoch : 495/1200, train_loss: 0.0344, test_loss: 0.0423, BestTest: 0.0186\n",
      "Epoch : 496/1200, train_loss: 0.0344, test_loss: 0.0247, BestTest: 0.0186\n",
      "Epoch : 497/1200, train_loss: 0.0356, test_loss: 0.0431, BestTest: 0.0186\n",
      "Epoch : 498/1200, train_loss: 0.0354, test_loss: 0.0400, BestTest: 0.0186\n",
      "Epoch : 499/1200, train_loss: 0.0339, test_loss: 0.0327, BestTest: 0.0186\n",
      "Epoch : 500/1200, train_loss: 0.0306, test_loss: 0.0346, BestTest: 0.0186\n",
      "Epoch : 501/1200, train_loss: 0.0298, test_loss: 0.0230, BestTest: 0.0186\n",
      "Epoch : 502/1200, train_loss: 0.0296, test_loss: 0.0280, BestTest: 0.0186\n",
      "Epoch : 503/1200, train_loss: 0.0299, test_loss: 0.0169, BestTest: 0.0169\n",
      "Epoch : 504/1200, train_loss: 0.0300, test_loss: 0.0293, BestTest: 0.0169\n",
      "Epoch : 505/1200, train_loss: 0.0300, test_loss: 0.0195, BestTest: 0.0169\n",
      "Epoch : 506/1200, train_loss: 0.0303, test_loss: 0.0233, BestTest: 0.0169\n",
      "Epoch : 507/1200, train_loss: 0.0297, test_loss: 0.0311, BestTest: 0.0169\n",
      "Epoch : 508/1200, train_loss: 0.0298, test_loss: 0.0193, BestTest: 0.0169\n",
      "Epoch : 509/1200, train_loss: 0.0280, test_loss: 0.0275, BestTest: 0.0169\n",
      "Epoch : 510/1200, train_loss: 0.0297, test_loss: 0.0270, BestTest: 0.0169\n",
      "Epoch : 511/1200, train_loss: 0.0297, test_loss: 0.0442, BestTest: 0.0169\n",
      "Epoch : 512/1200, train_loss: 0.0293, test_loss: 0.0252, BestTest: 0.0169\n",
      "Epoch : 513/1200, train_loss: 0.0290, test_loss: 0.0248, BestTest: 0.0169\n",
      "Epoch : 514/1200, train_loss: 0.0300, test_loss: 0.0284, BestTest: 0.0169\n",
      "Epoch : 515/1200, train_loss: 0.0291, test_loss: 0.0234, BestTest: 0.0169\n",
      "Epoch : 516/1200, train_loss: 0.0297, test_loss: 0.0253, BestTest: 0.0169\n",
      "Epoch : 517/1200, train_loss: 0.0291, test_loss: 0.0222, BestTest: 0.0169\n",
      "Epoch : 518/1200, train_loss: 0.0297, test_loss: 0.0256, BestTest: 0.0169\n",
      "Epoch : 519/1200, train_loss: 0.0302, test_loss: 0.0219, BestTest: 0.0169\n",
      "Epoch : 520/1200, train_loss: 0.0295, test_loss: 0.0254, BestTest: 0.0169\n",
      "Epoch : 521/1200, train_loss: 0.0285, test_loss: 0.0283, BestTest: 0.0169\n",
      "Epoch : 522/1200, train_loss: 0.0294, test_loss: 0.0232, BestTest: 0.0169\n",
      "Epoch : 523/1200, train_loss: 0.0299, test_loss: 0.0227, BestTest: 0.0169\n",
      "Epoch : 524/1200, train_loss: 0.0287, test_loss: 0.0220, BestTest: 0.0169\n",
      "Epoch : 525/1200, train_loss: 0.0291, test_loss: 0.0201, BestTest: 0.0169\n",
      "Epoch : 526/1200, train_loss: 0.0300, test_loss: 0.0240, BestTest: 0.0169\n",
      "Epoch : 527/1200, train_loss: 0.0291, test_loss: 0.0249, BestTest: 0.0169\n",
      "Epoch : 528/1200, train_loss: 0.0298, test_loss: 0.0239, BestTest: 0.0169\n",
      "Epoch : 529/1200, train_loss: 0.0296, test_loss: 0.0247, BestTest: 0.0169\n",
      "Epoch : 530/1200, train_loss: 0.0283, test_loss: 0.0222, BestTest: 0.0169\n",
      "Epoch : 531/1200, train_loss: 0.0293, test_loss: 0.0261, BestTest: 0.0169\n",
      "Epoch : 532/1200, train_loss: 0.0294, test_loss: 0.0167, BestTest: 0.0167\n",
      "Epoch : 533/1200, train_loss: 0.0288, test_loss: 0.0291, BestTest: 0.0167\n",
      "Epoch : 534/1200, train_loss: 0.0296, test_loss: 0.0190, BestTest: 0.0167\n",
      "Epoch : 535/1200, train_loss: 0.0291, test_loss: 0.0300, BestTest: 0.0167\n",
      "Epoch : 536/1200, train_loss: 0.0292, test_loss: 0.0219, BestTest: 0.0167\n",
      "Epoch : 537/1200, train_loss: 0.0296, test_loss: 0.0163, BestTest: 0.0163\n",
      "Epoch : 538/1200, train_loss: 0.0297, test_loss: 0.0224, BestTest: 0.0163\n",
      "Epoch : 539/1200, train_loss: 0.0285, test_loss: 0.0233, BestTest: 0.0163\n",
      "Epoch : 540/1200, train_loss: 0.0294, test_loss: 0.0291, BestTest: 0.0163\n",
      "Epoch : 541/1200, train_loss: 0.0288, test_loss: 0.0316, BestTest: 0.0163\n",
      "Epoch : 542/1200, train_loss: 0.0286, test_loss: 0.0173, BestTest: 0.0163\n",
      "Epoch : 543/1200, train_loss: 0.0285, test_loss: 0.0164, BestTest: 0.0163\n",
      "Epoch : 544/1200, train_loss: 0.0293, test_loss: 0.0269, BestTest: 0.0163\n",
      "Epoch : 545/1200, train_loss: 0.0294, test_loss: 0.0200, BestTest: 0.0163\n",
      "Epoch : 546/1200, train_loss: 0.0296, test_loss: 0.0251, BestTest: 0.0163\n",
      "Epoch : 547/1200, train_loss: 0.0299, test_loss: 0.0290, BestTest: 0.0163\n",
      "Epoch : 548/1200, train_loss: 0.0296, test_loss: 0.0247, BestTest: 0.0163\n",
      "Epoch : 549/1200, train_loss: 0.0306, test_loss: 0.0304, BestTest: 0.0163\n",
      "Epoch : 550/1200, train_loss: 0.0298, test_loss: 0.0286, BestTest: 0.0163\n",
      "Epoch : 551/1200, train_loss: 0.0281, test_loss: 0.0289, BestTest: 0.0163\n",
      "Epoch : 552/1200, train_loss: 0.0288, test_loss: 0.0346, BestTest: 0.0163\n",
      "Epoch : 553/1200, train_loss: 0.0285, test_loss: 0.0308, BestTest: 0.0163\n",
      "Epoch : 554/1200, train_loss: 0.0289, test_loss: 0.0224, BestTest: 0.0163\n",
      "Epoch : 555/1200, train_loss: 0.0293, test_loss: 0.0161, BestTest: 0.0161\n",
      "Epoch : 556/1200, train_loss: 0.0297, test_loss: 0.0200, BestTest: 0.0161\n",
      "Epoch : 557/1200, train_loss: 0.0292, test_loss: 0.0228, BestTest: 0.0161\n",
      "Epoch : 558/1200, train_loss: 0.0290, test_loss: 0.0222, BestTest: 0.0161\n",
      "Epoch : 559/1200, train_loss: 0.0292, test_loss: 0.0336, BestTest: 0.0161\n",
      "Epoch : 560/1200, train_loss: 0.0295, test_loss: 0.0234, BestTest: 0.0161\n",
      "Epoch : 561/1200, train_loss: 0.0285, test_loss: 0.0240, BestTest: 0.0161\n",
      "Epoch : 562/1200, train_loss: 0.0289, test_loss: 0.0224, BestTest: 0.0161\n",
      "Epoch : 563/1200, train_loss: 0.0294, test_loss: 0.0292, BestTest: 0.0161\n",
      "Epoch : 564/1200, train_loss: 0.0290, test_loss: 0.0196, BestTest: 0.0161\n",
      "Epoch : 565/1200, train_loss: 0.0289, test_loss: 0.0293, BestTest: 0.0161\n",
      "Epoch : 566/1200, train_loss: 0.0288, test_loss: 0.0236, BestTest: 0.0161\n",
      "Epoch : 567/1200, train_loss: 0.0292, test_loss: 0.0369, BestTest: 0.0161\n",
      "Epoch : 568/1200, train_loss: 0.0290, test_loss: 0.0188, BestTest: 0.0161\n",
      "Epoch : 569/1200, train_loss: 0.0291, test_loss: 0.0209, BestTest: 0.0161\n",
      "Epoch : 570/1200, train_loss: 0.0292, test_loss: 0.0342, BestTest: 0.0161\n",
      "Epoch : 571/1200, train_loss: 0.0292, test_loss: 0.0327, BestTest: 0.0161\n",
      "Epoch : 572/1200, train_loss: 0.0290, test_loss: 0.0246, BestTest: 0.0161\n",
      "Epoch : 573/1200, train_loss: 0.0289, test_loss: 0.0185, BestTest: 0.0161\n",
      "Epoch : 574/1200, train_loss: 0.0289, test_loss: 0.0186, BestTest: 0.0161\n",
      "Epoch : 575/1200, train_loss: 0.0285, test_loss: 0.0265, BestTest: 0.0161\n",
      "Epoch : 576/1200, train_loss: 0.0286, test_loss: 0.0257, BestTest: 0.0161\n",
      "Epoch : 577/1200, train_loss: 0.0285, test_loss: 0.0238, BestTest: 0.0161\n",
      "Epoch : 578/1200, train_loss: 0.0284, test_loss: 0.0183, BestTest: 0.0161\n",
      "Epoch : 579/1200, train_loss: 0.0292, test_loss: 0.0292, BestTest: 0.0161\n",
      "Epoch : 580/1200, train_loss: 0.0294, test_loss: 0.0145, BestTest: 0.0145\n",
      "Epoch : 581/1200, train_loss: 0.0292, test_loss: 0.0236, BestTest: 0.0145\n",
      "Epoch : 582/1200, train_loss: 0.0286, test_loss: 0.0257, BestTest: 0.0145\n",
      "Epoch : 583/1200, train_loss: 0.0286, test_loss: 0.0257, BestTest: 0.0145\n",
      "Epoch : 584/1200, train_loss: 0.0286, test_loss: 0.0265, BestTest: 0.0145\n",
      "Epoch : 585/1200, train_loss: 0.0289, test_loss: 0.0177, BestTest: 0.0145\n",
      "Epoch : 586/1200, train_loss: 0.0283, test_loss: 0.0249, BestTest: 0.0145\n",
      "Epoch : 587/1200, train_loss: 0.0287, test_loss: 0.0203, BestTest: 0.0145\n",
      "Epoch : 588/1200, train_loss: 0.0291, test_loss: 0.0391, BestTest: 0.0145\n",
      "Epoch : 589/1200, train_loss: 0.0295, test_loss: 0.0272, BestTest: 0.0145\n",
      "Epoch : 590/1200, train_loss: 0.0283, test_loss: 0.0256, BestTest: 0.0145\n",
      "Epoch : 591/1200, train_loss: 0.0292, test_loss: 0.0250, BestTest: 0.0145\n",
      "Epoch : 592/1200, train_loss: 0.0293, test_loss: 0.0165, BestTest: 0.0145\n",
      "Epoch : 593/1200, train_loss: 0.0286, test_loss: 0.0213, BestTest: 0.0145\n",
      "Epoch : 594/1200, train_loss: 0.0286, test_loss: 0.0218, BestTest: 0.0145\n",
      "Epoch : 595/1200, train_loss: 0.0286, test_loss: 0.0235, BestTest: 0.0145\n",
      "Epoch : 596/1200, train_loss: 0.0289, test_loss: 0.0193, BestTest: 0.0145\n",
      "Epoch : 597/1200, train_loss: 0.0288, test_loss: 0.0171, BestTest: 0.0145\n",
      "Epoch : 598/1200, train_loss: 0.0282, test_loss: 0.0327, BestTest: 0.0145\n",
      "Epoch : 599/1200, train_loss: 0.0293, test_loss: 0.0256, BestTest: 0.0145\n",
      "Epoch : 600/1200, train_loss: 0.0263, test_loss: 0.0199, BestTest: 0.0145\n",
      "Epoch : 601/1200, train_loss: 0.0262, test_loss: 0.0212, BestTest: 0.0145\n",
      "Epoch : 602/1200, train_loss: 0.0262, test_loss: 0.0189, BestTest: 0.0145\n",
      "Epoch : 603/1200, train_loss: 0.0257, test_loss: 0.0161, BestTest: 0.0145\n",
      "Epoch : 604/1200, train_loss: 0.0260, test_loss: 0.0189, BestTest: 0.0145\n",
      "Epoch : 605/1200, train_loss: 0.0261, test_loss: 0.0205, BestTest: 0.0145\n",
      "Epoch : 606/1200, train_loss: 0.0262, test_loss: 0.0303, BestTest: 0.0145\n",
      "Epoch : 607/1200, train_loss: 0.0263, test_loss: 0.0201, BestTest: 0.0145\n",
      "Epoch : 608/1200, train_loss: 0.0256, test_loss: 0.0240, BestTest: 0.0145\n",
      "Epoch : 609/1200, train_loss: 0.0263, test_loss: 0.0213, BestTest: 0.0145\n",
      "Epoch : 610/1200, train_loss: 0.0261, test_loss: 0.0128, BestTest: 0.0128\n",
      "Epoch : 611/1200, train_loss: 0.0260, test_loss: 0.0248, BestTest: 0.0128\n",
      "Epoch : 612/1200, train_loss: 0.0264, test_loss: 0.0174, BestTest: 0.0128\n",
      "Epoch : 613/1200, train_loss: 0.0260, test_loss: 0.0195, BestTest: 0.0128\n",
      "Epoch : 614/1200, train_loss: 0.0269, test_loss: 0.0206, BestTest: 0.0128\n",
      "Epoch : 615/1200, train_loss: 0.0256, test_loss: 0.0296, BestTest: 0.0128\n",
      "Epoch : 616/1200, train_loss: 0.0257, test_loss: 0.0167, BestTest: 0.0128\n",
      "Epoch : 617/1200, train_loss: 0.0260, test_loss: 0.0185, BestTest: 0.0128\n",
      "Epoch : 618/1200, train_loss: 0.0261, test_loss: 0.0169, BestTest: 0.0128\n",
      "Epoch : 619/1200, train_loss: 0.0256, test_loss: 0.0255, BestTest: 0.0128\n",
      "Epoch : 620/1200, train_loss: 0.0261, test_loss: 0.0146, BestTest: 0.0128\n",
      "Epoch : 621/1200, train_loss: 0.0259, test_loss: 0.0207, BestTest: 0.0128\n",
      "Epoch : 622/1200, train_loss: 0.0257, test_loss: 0.0194, BestTest: 0.0128\n",
      "Epoch : 623/1200, train_loss: 0.0263, test_loss: 0.0301, BestTest: 0.0128\n",
      "Epoch : 624/1200, train_loss: 0.0259, test_loss: 0.0203, BestTest: 0.0128\n",
      "Epoch : 625/1200, train_loss: 0.0261, test_loss: 0.0268, BestTest: 0.0128\n",
      "Epoch : 626/1200, train_loss: 0.0256, test_loss: 0.0184, BestTest: 0.0128\n",
      "Epoch : 627/1200, train_loss: 0.0262, test_loss: 0.0202, BestTest: 0.0128\n",
      "Epoch : 628/1200, train_loss: 0.0264, test_loss: 0.0248, BestTest: 0.0128\n",
      "Epoch : 629/1200, train_loss: 0.0272, test_loss: 0.0228, BestTest: 0.0128\n",
      "Epoch : 630/1200, train_loss: 0.0261, test_loss: 0.0225, BestTest: 0.0128\n",
      "Epoch : 631/1200, train_loss: 0.0251, test_loss: 0.0168, BestTest: 0.0128\n",
      "Epoch : 632/1200, train_loss: 0.0260, test_loss: 0.0234, BestTest: 0.0128\n",
      "Epoch : 633/1200, train_loss: 0.0262, test_loss: 0.0199, BestTest: 0.0128\n",
      "Epoch : 634/1200, train_loss: 0.0265, test_loss: 0.0243, BestTest: 0.0128\n",
      "Epoch : 635/1200, train_loss: 0.0255, test_loss: 0.0173, BestTest: 0.0128\n",
      "Epoch : 636/1200, train_loss: 0.0258, test_loss: 0.0211, BestTest: 0.0128\n",
      "Epoch : 637/1200, train_loss: 0.0263, test_loss: 0.0202, BestTest: 0.0128\n",
      "Epoch : 638/1200, train_loss: 0.0262, test_loss: 0.0204, BestTest: 0.0128\n",
      "Epoch : 639/1200, train_loss: 0.0262, test_loss: 0.0266, BestTest: 0.0128\n",
      "Epoch : 640/1200, train_loss: 0.0259, test_loss: 0.0228, BestTest: 0.0128\n",
      "Epoch : 641/1200, train_loss: 0.0260, test_loss: 0.0217, BestTest: 0.0128\n",
      "Epoch : 642/1200, train_loss: 0.0252, test_loss: 0.0227, BestTest: 0.0128\n",
      "Epoch : 643/1200, train_loss: 0.0257, test_loss: 0.0233, BestTest: 0.0128\n",
      "Epoch : 644/1200, train_loss: 0.0259, test_loss: 0.0240, BestTest: 0.0128\n",
      "Epoch : 645/1200, train_loss: 0.0255, test_loss: 0.0207, BestTest: 0.0128\n",
      "Epoch : 646/1200, train_loss: 0.0258, test_loss: 0.0250, BestTest: 0.0128\n",
      "Epoch : 647/1200, train_loss: 0.0260, test_loss: 0.0174, BestTest: 0.0128\n",
      "Epoch : 648/1200, train_loss: 0.0259, test_loss: 0.0217, BestTest: 0.0128\n",
      "Epoch : 649/1200, train_loss: 0.0262, test_loss: 0.0259, BestTest: 0.0128\n",
      "Epoch : 650/1200, train_loss: 0.0268, test_loss: 0.0264, BestTest: 0.0128\n",
      "Epoch : 651/1200, train_loss: 0.0259, test_loss: 0.0231, BestTest: 0.0128\n",
      "Epoch : 652/1200, train_loss: 0.0259, test_loss: 0.0186, BestTest: 0.0128\n",
      "Epoch : 653/1200, train_loss: 0.0256, test_loss: 0.0188, BestTest: 0.0128\n",
      "Epoch : 654/1200, train_loss: 0.0259, test_loss: 0.0182, BestTest: 0.0128\n",
      "Epoch : 655/1200, train_loss: 0.0258, test_loss: 0.0350, BestTest: 0.0128\n",
      "Epoch : 656/1200, train_loss: 0.0261, test_loss: 0.0252, BestTest: 0.0128\n",
      "Epoch : 657/1200, train_loss: 0.0255, test_loss: 0.0228, BestTest: 0.0128\n",
      "Epoch : 658/1200, train_loss: 0.0261, test_loss: 0.0127, BestTest: 0.0127\n",
      "Epoch : 659/1200, train_loss: 0.0260, test_loss: 0.0234, BestTest: 0.0127\n",
      "Epoch : 660/1200, train_loss: 0.0259, test_loss: 0.0187, BestTest: 0.0127\n",
      "Epoch : 661/1200, train_loss: 0.0259, test_loss: 0.0225, BestTest: 0.0127\n",
      "Epoch : 662/1200, train_loss: 0.0261, test_loss: 0.0208, BestTest: 0.0127\n",
      "Epoch : 663/1200, train_loss: 0.0256, test_loss: 0.0255, BestTest: 0.0127\n",
      "Epoch : 664/1200, train_loss: 0.0257, test_loss: 0.0252, BestTest: 0.0127\n",
      "Epoch : 665/1200, train_loss: 0.0263, test_loss: 0.0163, BestTest: 0.0127\n",
      "Epoch : 666/1200, train_loss: 0.0266, test_loss: 0.0208, BestTest: 0.0127\n",
      "Epoch : 667/1200, train_loss: 0.0264, test_loss: 0.0246, BestTest: 0.0127\n",
      "Epoch : 668/1200, train_loss: 0.0256, test_loss: 0.0165, BestTest: 0.0127\n",
      "Epoch : 669/1200, train_loss: 0.0265, test_loss: 0.0146, BestTest: 0.0127\n",
      "Epoch : 670/1200, train_loss: 0.0261, test_loss: 0.0256, BestTest: 0.0127\n",
      "Epoch : 671/1200, train_loss: 0.0256, test_loss: 0.0268, BestTest: 0.0127\n",
      "Epoch : 672/1200, train_loss: 0.0255, test_loss: 0.0246, BestTest: 0.0127\n",
      "Epoch : 673/1200, train_loss: 0.0258, test_loss: 0.0198, BestTest: 0.0127\n",
      "Epoch : 674/1200, train_loss: 0.0256, test_loss: 0.0236, BestTest: 0.0127\n",
      "Epoch : 675/1200, train_loss: 0.0261, test_loss: 0.0225, BestTest: 0.0127\n",
      "Epoch : 676/1200, train_loss: 0.0254, test_loss: 0.0213, BestTest: 0.0127\n",
      "Epoch : 677/1200, train_loss: 0.0261, test_loss: 0.0170, BestTest: 0.0127\n",
      "Epoch : 678/1200, train_loss: 0.0258, test_loss: 0.0167, BestTest: 0.0127\n",
      "Epoch : 679/1200, train_loss: 0.0266, test_loss: 0.0228, BestTest: 0.0127\n",
      "Epoch : 680/1200, train_loss: 0.0265, test_loss: 0.0213, BestTest: 0.0127\n",
      "Epoch : 681/1200, train_loss: 0.0259, test_loss: 0.0199, BestTest: 0.0127\n",
      "Epoch : 682/1200, train_loss: 0.0258, test_loss: 0.0153, BestTest: 0.0127\n",
      "Epoch : 683/1200, train_loss: 0.0252, test_loss: 0.0242, BestTest: 0.0127\n",
      "Epoch : 684/1200, train_loss: 0.0257, test_loss: 0.0247, BestTest: 0.0127\n",
      "Epoch : 685/1200, train_loss: 0.0253, test_loss: 0.0207, BestTest: 0.0127\n",
      "Epoch : 686/1200, train_loss: 0.0254, test_loss: 0.0264, BestTest: 0.0127\n",
      "Epoch : 687/1200, train_loss: 0.0260, test_loss: 0.0214, BestTest: 0.0127\n",
      "Epoch : 688/1200, train_loss: 0.0257, test_loss: 0.0186, BestTest: 0.0127\n",
      "Epoch : 689/1200, train_loss: 0.0256, test_loss: 0.0286, BestTest: 0.0127\n",
      "Epoch : 690/1200, train_loss: 0.0259, test_loss: 0.0258, BestTest: 0.0127\n",
      "Epoch : 691/1200, train_loss: 0.0259, test_loss: 0.0179, BestTest: 0.0127\n",
      "Epoch : 692/1200, train_loss: 0.0256, test_loss: 0.0295, BestTest: 0.0127\n",
      "Epoch : 693/1200, train_loss: 0.0258, test_loss: 0.0212, BestTest: 0.0127\n",
      "Epoch : 694/1200, train_loss: 0.0259, test_loss: 0.0262, BestTest: 0.0127\n",
      "Epoch : 695/1200, train_loss: 0.0259, test_loss: 0.0204, BestTest: 0.0127\n",
      "Epoch : 696/1200, train_loss: 0.0258, test_loss: 0.0216, BestTest: 0.0127\n",
      "Epoch : 697/1200, train_loss: 0.0257, test_loss: 0.0151, BestTest: 0.0127\n",
      "Epoch : 698/1200, train_loss: 0.0259, test_loss: 0.0178, BestTest: 0.0127\n",
      "Epoch : 699/1200, train_loss: 0.0251, test_loss: 0.0202, BestTest: 0.0127\n",
      "Epoch : 700/1200, train_loss: 0.0247, test_loss: 0.0178, BestTest: 0.0127\n",
      "Epoch : 701/1200, train_loss: 0.0249, test_loss: 0.0206, BestTest: 0.0127\n",
      "Epoch : 702/1200, train_loss: 0.0240, test_loss: 0.0213, BestTest: 0.0127\n",
      "Epoch : 703/1200, train_loss: 0.0239, test_loss: 0.0210, BestTest: 0.0127\n",
      "Epoch : 704/1200, train_loss: 0.0241, test_loss: 0.0206, BestTest: 0.0127\n",
      "Epoch : 705/1200, train_loss: 0.0241, test_loss: 0.0205, BestTest: 0.0127\n",
      "Epoch : 706/1200, train_loss: 0.0241, test_loss: 0.0173, BestTest: 0.0127\n",
      "Epoch : 707/1200, train_loss: 0.0243, test_loss: 0.0144, BestTest: 0.0127\n",
      "Epoch : 708/1200, train_loss: 0.0244, test_loss: 0.0175, BestTest: 0.0127\n",
      "Epoch : 709/1200, train_loss: 0.0236, test_loss: 0.0222, BestTest: 0.0127\n",
      "Epoch : 710/1200, train_loss: 0.0243, test_loss: 0.0225, BestTest: 0.0127\n",
      "Epoch : 711/1200, train_loss: 0.0241, test_loss: 0.0187, BestTest: 0.0127\n",
      "Epoch : 712/1200, train_loss: 0.0244, test_loss: 0.0160, BestTest: 0.0127\n",
      "Epoch : 713/1200, train_loss: 0.0239, test_loss: 0.0213, BestTest: 0.0127\n",
      "Epoch : 714/1200, train_loss: 0.0244, test_loss: 0.0214, BestTest: 0.0127\n",
      "Epoch : 715/1200, train_loss: 0.0239, test_loss: 0.0202, BestTest: 0.0127\n",
      "Epoch : 716/1200, train_loss: 0.0242, test_loss: 0.0217, BestTest: 0.0127\n",
      "Epoch : 717/1200, train_loss: 0.0241, test_loss: 0.0164, BestTest: 0.0127\n",
      "Epoch : 718/1200, train_loss: 0.0243, test_loss: 0.0214, BestTest: 0.0127\n",
      "Epoch : 719/1200, train_loss: 0.0244, test_loss: 0.0202, BestTest: 0.0127\n",
      "Epoch : 720/1200, train_loss: 0.0243, test_loss: 0.0199, BestTest: 0.0127\n",
      "Epoch : 721/1200, train_loss: 0.0244, test_loss: 0.0196, BestTest: 0.0127\n",
      "Epoch : 722/1200, train_loss: 0.0240, test_loss: 0.0165, BestTest: 0.0127\n",
      "Epoch : 723/1200, train_loss: 0.0242, test_loss: 0.0192, BestTest: 0.0127\n",
      "Epoch : 724/1200, train_loss: 0.0247, test_loss: 0.0239, BestTest: 0.0127\n",
      "Epoch : 725/1200, train_loss: 0.0242, test_loss: 0.0202, BestTest: 0.0127\n",
      "Epoch : 726/1200, train_loss: 0.0240, test_loss: 0.0189, BestTest: 0.0127\n",
      "Epoch : 727/1200, train_loss: 0.0242, test_loss: 0.0167, BestTest: 0.0127\n",
      "Epoch : 728/1200, train_loss: 0.0243, test_loss: 0.0257, BestTest: 0.0127\n",
      "Epoch : 729/1200, train_loss: 0.0236, test_loss: 0.0221, BestTest: 0.0127\n",
      "Epoch : 730/1200, train_loss: 0.0242, test_loss: 0.0246, BestTest: 0.0127\n",
      "Epoch : 731/1200, train_loss: 0.0240, test_loss: 0.0196, BestTest: 0.0127\n",
      "Epoch : 732/1200, train_loss: 0.0241, test_loss: 0.0170, BestTest: 0.0127\n",
      "Epoch : 733/1200, train_loss: 0.0244, test_loss: 0.0236, BestTest: 0.0127\n",
      "Epoch : 734/1200, train_loss: 0.0242, test_loss: 0.0184, BestTest: 0.0127\n",
      "Epoch : 735/1200, train_loss: 0.0244, test_loss: 0.0255, BestTest: 0.0127\n",
      "Epoch : 736/1200, train_loss: 0.0240, test_loss: 0.0206, BestTest: 0.0127\n",
      "Epoch : 737/1200, train_loss: 0.0242, test_loss: 0.0246, BestTest: 0.0127\n",
      "Epoch : 738/1200, train_loss: 0.0239, test_loss: 0.0203, BestTest: 0.0127\n",
      "Epoch : 739/1200, train_loss: 0.0241, test_loss: 0.0269, BestTest: 0.0127\n",
      "Epoch : 740/1200, train_loss: 0.0242, test_loss: 0.0194, BestTest: 0.0127\n",
      "Epoch : 741/1200, train_loss: 0.0243, test_loss: 0.0189, BestTest: 0.0127\n",
      "Epoch : 742/1200, train_loss: 0.0242, test_loss: 0.0175, BestTest: 0.0127\n",
      "Epoch : 743/1200, train_loss: 0.0241, test_loss: 0.0231, BestTest: 0.0127\n",
      "Epoch : 744/1200, train_loss: 0.0244, test_loss: 0.0212, BestTest: 0.0127\n",
      "Epoch : 745/1200, train_loss: 0.0237, test_loss: 0.0166, BestTest: 0.0127\n",
      "Epoch : 746/1200, train_loss: 0.0245, test_loss: 0.0190, BestTest: 0.0127\n",
      "Epoch : 747/1200, train_loss: 0.0237, test_loss: 0.0164, BestTest: 0.0127\n",
      "Epoch : 748/1200, train_loss: 0.0241, test_loss: 0.0250, BestTest: 0.0127\n",
      "Epoch : 749/1200, train_loss: 0.0239, test_loss: 0.0186, BestTest: 0.0127\n",
      "Epoch : 750/1200, train_loss: 0.0246, test_loss: 0.0205, BestTest: 0.0127\n",
      "Epoch : 751/1200, train_loss: 0.0240, test_loss: 0.0180, BestTest: 0.0127\n",
      "Epoch : 752/1200, train_loss: 0.0243, test_loss: 0.0164, BestTest: 0.0127\n",
      "Epoch : 753/1200, train_loss: 0.0245, test_loss: 0.0217, BestTest: 0.0127\n",
      "Epoch : 754/1200, train_loss: 0.0243, test_loss: 0.0193, BestTest: 0.0127\n",
      "Epoch : 755/1200, train_loss: 0.0238, test_loss: 0.0216, BestTest: 0.0127\n",
      "Epoch : 756/1200, train_loss: 0.0240, test_loss: 0.0158, BestTest: 0.0127\n",
      "Epoch : 757/1200, train_loss: 0.0244, test_loss: 0.0229, BestTest: 0.0127\n",
      "Epoch : 758/1200, train_loss: 0.0242, test_loss: 0.0225, BestTest: 0.0127\n",
      "Epoch : 759/1200, train_loss: 0.0237, test_loss: 0.0165, BestTest: 0.0127\n",
      "Epoch : 760/1200, train_loss: 0.0240, test_loss: 0.0199, BestTest: 0.0127\n",
      "Epoch : 761/1200, train_loss: 0.0244, test_loss: 0.0198, BestTest: 0.0127\n",
      "Epoch : 762/1200, train_loss: 0.0245, test_loss: 0.0240, BestTest: 0.0127\n",
      "Epoch : 763/1200, train_loss: 0.0240, test_loss: 0.0206, BestTest: 0.0127\n",
      "Epoch : 764/1200, train_loss: 0.0236, test_loss: 0.0221, BestTest: 0.0127\n",
      "Epoch : 765/1200, train_loss: 0.0243, test_loss: 0.0171, BestTest: 0.0127\n",
      "Epoch : 766/1200, train_loss: 0.0240, test_loss: 0.0193, BestTest: 0.0127\n",
      "Epoch : 767/1200, train_loss: 0.0243, test_loss: 0.0235, BestTest: 0.0127\n",
      "Epoch : 768/1200, train_loss: 0.0242, test_loss: 0.0205, BestTest: 0.0127\n",
      "Epoch : 769/1200, train_loss: 0.0245, test_loss: 0.0224, BestTest: 0.0127\n",
      "Epoch : 770/1200, train_loss: 0.0239, test_loss: 0.0189, BestTest: 0.0127\n",
      "Epoch : 771/1200, train_loss: 0.0239, test_loss: 0.0192, BestTest: 0.0127\n",
      "Epoch : 772/1200, train_loss: 0.0238, test_loss: 0.0168, BestTest: 0.0127\n",
      "Epoch : 773/1200, train_loss: 0.0242, test_loss: 0.0198, BestTest: 0.0127\n",
      "Epoch : 774/1200, train_loss: 0.0245, test_loss: 0.0225, BestTest: 0.0127\n",
      "Epoch : 775/1200, train_loss: 0.0240, test_loss: 0.0222, BestTest: 0.0127\n",
      "Epoch : 776/1200, train_loss: 0.0238, test_loss: 0.0237, BestTest: 0.0127\n",
      "Epoch : 777/1200, train_loss: 0.0239, test_loss: 0.0167, BestTest: 0.0127\n",
      "Epoch : 778/1200, train_loss: 0.0237, test_loss: 0.0224, BestTest: 0.0127\n",
      "Epoch : 779/1200, train_loss: 0.0239, test_loss: 0.0160, BestTest: 0.0127\n",
      "Epoch : 780/1200, train_loss: 0.0246, test_loss: 0.0175, BestTest: 0.0127\n",
      "Epoch : 781/1200, train_loss: 0.0242, test_loss: 0.0197, BestTest: 0.0127\n",
      "Epoch : 782/1200, train_loss: 0.0242, test_loss: 0.0175, BestTest: 0.0127\n",
      "Epoch : 783/1200, train_loss: 0.0243, test_loss: 0.0241, BestTest: 0.0127\n",
      "Epoch : 784/1200, train_loss: 0.0239, test_loss: 0.0202, BestTest: 0.0127\n",
      "Epoch : 785/1200, train_loss: 0.0240, test_loss: 0.0187, BestTest: 0.0127\n",
      "Epoch : 786/1200, train_loss: 0.0242, test_loss: 0.0183, BestTest: 0.0127\n",
      "Epoch : 787/1200, train_loss: 0.0246, test_loss: 0.0182, BestTest: 0.0127\n",
      "Epoch : 788/1200, train_loss: 0.0239, test_loss: 0.0177, BestTest: 0.0127\n",
      "Epoch : 789/1200, train_loss: 0.0241, test_loss: 0.0215, BestTest: 0.0127\n",
      "Epoch : 790/1200, train_loss: 0.0241, test_loss: 0.0206, BestTest: 0.0127\n",
      "Epoch : 791/1200, train_loss: 0.0242, test_loss: 0.0202, BestTest: 0.0127\n",
      "Epoch : 792/1200, train_loss: 0.0243, test_loss: 0.0225, BestTest: 0.0127\n",
      "Epoch : 793/1200, train_loss: 0.0241, test_loss: 0.0189, BestTest: 0.0127\n",
      "Epoch : 794/1200, train_loss: 0.0238, test_loss: 0.0209, BestTest: 0.0127\n",
      "Epoch : 795/1200, train_loss: 0.0244, test_loss: 0.0163, BestTest: 0.0127\n",
      "Epoch : 796/1200, train_loss: 0.0241, test_loss: 0.0180, BestTest: 0.0127\n",
      "Epoch : 797/1200, train_loss: 0.0240, test_loss: 0.0198, BestTest: 0.0127\n",
      "Epoch : 798/1200, train_loss: 0.0240, test_loss: 0.0203, BestTest: 0.0127\n",
      "Epoch : 799/1200, train_loss: 0.0238, test_loss: 0.0208, BestTest: 0.0127\n",
      "Epoch : 800/1200, train_loss: 0.0233, test_loss: 0.0213, BestTest: 0.0127\n",
      "Epoch : 801/1200, train_loss: 0.0234, test_loss: 0.0169, BestTest: 0.0127\n",
      "Epoch : 802/1200, train_loss: 0.0233, test_loss: 0.0189, BestTest: 0.0127\n",
      "Epoch : 803/1200, train_loss: 0.0228, test_loss: 0.0206, BestTest: 0.0127\n",
      "Epoch : 804/1200, train_loss: 0.0233, test_loss: 0.0177, BestTest: 0.0127\n",
      "Epoch : 805/1200, train_loss: 0.0227, test_loss: 0.0243, BestTest: 0.0127\n",
      "Epoch : 806/1200, train_loss: 0.0233, test_loss: 0.0174, BestTest: 0.0127\n",
      "Epoch : 807/1200, train_loss: 0.0233, test_loss: 0.0192, BestTest: 0.0127\n",
      "Epoch : 808/1200, train_loss: 0.0231, test_loss: 0.0222, BestTest: 0.0127\n",
      "Epoch : 809/1200, train_loss: 0.0232, test_loss: 0.0178, BestTest: 0.0127\n",
      "Epoch : 810/1200, train_loss: 0.0234, test_loss: 0.0210, BestTest: 0.0127\n",
      "Epoch : 811/1200, train_loss: 0.0231, test_loss: 0.0217, BestTest: 0.0127\n",
      "Epoch : 812/1200, train_loss: 0.0230, test_loss: 0.0212, BestTest: 0.0127\n",
      "Epoch : 813/1200, train_loss: 0.0233, test_loss: 0.0177, BestTest: 0.0127\n",
      "Epoch : 814/1200, train_loss: 0.0234, test_loss: 0.0203, BestTest: 0.0127\n",
      "Epoch : 815/1200, train_loss: 0.0230, test_loss: 0.0208, BestTest: 0.0127\n",
      "Epoch : 816/1200, train_loss: 0.0232, test_loss: 0.0226, BestTest: 0.0127\n",
      "Epoch : 817/1200, train_loss: 0.0228, test_loss: 0.0176, BestTest: 0.0127\n",
      "Epoch : 818/1200, train_loss: 0.0234, test_loss: 0.0164, BestTest: 0.0127\n",
      "Epoch : 819/1200, train_loss: 0.0238, test_loss: 0.0210, BestTest: 0.0127\n",
      "Epoch : 820/1200, train_loss: 0.0231, test_loss: 0.0190, BestTest: 0.0127\n",
      "Epoch : 821/1200, train_loss: 0.0230, test_loss: 0.0195, BestTest: 0.0127\n",
      "Epoch : 822/1200, train_loss: 0.0236, test_loss: 0.0194, BestTest: 0.0127\n",
      "Epoch : 823/1200, train_loss: 0.0230, test_loss: 0.0179, BestTest: 0.0127\n",
      "Epoch : 824/1200, train_loss: 0.0227, test_loss: 0.0204, BestTest: 0.0127\n",
      "Epoch : 825/1200, train_loss: 0.0233, test_loss: 0.0265, BestTest: 0.0127\n",
      "Epoch : 826/1200, train_loss: 0.0234, test_loss: 0.0198, BestTest: 0.0127\n",
      "Epoch : 827/1200, train_loss: 0.0231, test_loss: 0.0188, BestTest: 0.0127\n",
      "Epoch : 828/1200, train_loss: 0.0233, test_loss: 0.0178, BestTest: 0.0127\n",
      "Epoch : 829/1200, train_loss: 0.0229, test_loss: 0.0195, BestTest: 0.0127\n",
      "Epoch : 830/1200, train_loss: 0.0234, test_loss: 0.0191, BestTest: 0.0127\n",
      "Epoch : 831/1200, train_loss: 0.0232, test_loss: 0.0187, BestTest: 0.0127\n",
      "Epoch : 832/1200, train_loss: 0.0230, test_loss: 0.0197, BestTest: 0.0127\n",
      "Epoch : 833/1200, train_loss: 0.0235, test_loss: 0.0209, BestTest: 0.0127\n",
      "Epoch : 834/1200, train_loss: 0.0228, test_loss: 0.0187, BestTest: 0.0127\n",
      "Epoch : 835/1200, train_loss: 0.0230, test_loss: 0.0207, BestTest: 0.0127\n",
      "Epoch : 836/1200, train_loss: 0.0232, test_loss: 0.0197, BestTest: 0.0127\n",
      "Epoch : 837/1200, train_loss: 0.0233, test_loss: 0.0205, BestTest: 0.0127\n",
      "Epoch : 838/1200, train_loss: 0.0232, test_loss: 0.0198, BestTest: 0.0127\n",
      "Epoch : 839/1200, train_loss: 0.0230, test_loss: 0.0227, BestTest: 0.0127\n",
      "Epoch : 840/1200, train_loss: 0.0234, test_loss: 0.0220, BestTest: 0.0127\n",
      "Epoch : 841/1200, train_loss: 0.0229, test_loss: 0.0185, BestTest: 0.0127\n",
      "Epoch : 842/1200, train_loss: 0.0237, test_loss: 0.0217, BestTest: 0.0127\n",
      "Epoch : 843/1200, train_loss: 0.0235, test_loss: 0.0191, BestTest: 0.0127\n",
      "Epoch : 844/1200, train_loss: 0.0233, test_loss: 0.0240, BestTest: 0.0127\n",
      "Epoch : 845/1200, train_loss: 0.0231, test_loss: 0.0216, BestTest: 0.0127\n",
      "Epoch : 846/1200, train_loss: 0.0235, test_loss: 0.0203, BestTest: 0.0127\n",
      "Epoch : 847/1200, train_loss: 0.0233, test_loss: 0.0196, BestTest: 0.0127\n",
      "Epoch : 848/1200, train_loss: 0.0234, test_loss: 0.0214, BestTest: 0.0127\n",
      "Epoch : 849/1200, train_loss: 0.0229, test_loss: 0.0187, BestTest: 0.0127\n",
      "Epoch : 850/1200, train_loss: 0.0230, test_loss: 0.0199, BestTest: 0.0127\n",
      "Epoch : 851/1200, train_loss: 0.0234, test_loss: 0.0169, BestTest: 0.0127\n",
      "Epoch : 852/1200, train_loss: 0.0235, test_loss: 0.0212, BestTest: 0.0127\n",
      "Epoch : 853/1200, train_loss: 0.0232, test_loss: 0.0254, BestTest: 0.0127\n",
      "Epoch : 854/1200, train_loss: 0.0230, test_loss: 0.0204, BestTest: 0.0127\n",
      "Epoch : 855/1200, train_loss: 0.0234, test_loss: 0.0200, BestTest: 0.0127\n",
      "Epoch : 856/1200, train_loss: 0.0236, test_loss: 0.0178, BestTest: 0.0127\n",
      "Epoch : 857/1200, train_loss: 0.0236, test_loss: 0.0213, BestTest: 0.0127\n",
      "Epoch : 858/1200, train_loss: 0.0233, test_loss: 0.0176, BestTest: 0.0127\n",
      "Epoch : 859/1200, train_loss: 0.0235, test_loss: 0.0196, BestTest: 0.0127\n",
      "Epoch : 860/1200, train_loss: 0.0235, test_loss: 0.0205, BestTest: 0.0127\n",
      "Epoch : 861/1200, train_loss: 0.0233, test_loss: 0.0224, BestTest: 0.0127\n",
      "Epoch : 862/1200, train_loss: 0.0233, test_loss: 0.0173, BestTest: 0.0127\n",
      "Epoch : 863/1200, train_loss: 0.0231, test_loss: 0.0143, BestTest: 0.0127\n",
      "Epoch : 864/1200, train_loss: 0.0231, test_loss: 0.0190, BestTest: 0.0127\n",
      "Epoch : 865/1200, train_loss: 0.0229, test_loss: 0.0208, BestTest: 0.0127\n",
      "Epoch : 866/1200, train_loss: 0.0230, test_loss: 0.0243, BestTest: 0.0127\n",
      "Epoch : 867/1200, train_loss: 0.0233, test_loss: 0.0201, BestTest: 0.0127\n",
      "Epoch : 868/1200, train_loss: 0.0229, test_loss: 0.0204, BestTest: 0.0127\n",
      "Epoch : 869/1200, train_loss: 0.0236, test_loss: 0.0198, BestTest: 0.0127\n",
      "Epoch : 870/1200, train_loss: 0.0231, test_loss: 0.0199, BestTest: 0.0127\n",
      "Epoch : 871/1200, train_loss: 0.0234, test_loss: 0.0213, BestTest: 0.0127\n",
      "Epoch : 872/1200, train_loss: 0.0226, test_loss: 0.0210, BestTest: 0.0127\n",
      "Epoch : 873/1200, train_loss: 0.0231, test_loss: 0.0181, BestTest: 0.0127\n",
      "Epoch : 874/1200, train_loss: 0.0232, test_loss: 0.0213, BestTest: 0.0127\n",
      "Epoch : 875/1200, train_loss: 0.0236, test_loss: 0.0207, BestTest: 0.0127\n",
      "Epoch : 876/1200, train_loss: 0.0231, test_loss: 0.0177, BestTest: 0.0127\n",
      "Epoch : 877/1200, train_loss: 0.0228, test_loss: 0.0194, BestTest: 0.0127\n",
      "Epoch : 878/1200, train_loss: 0.0230, test_loss: 0.0228, BestTest: 0.0127\n",
      "Epoch : 879/1200, train_loss: 0.0232, test_loss: 0.0232, BestTest: 0.0127\n",
      "Epoch : 880/1200, train_loss: 0.0232, test_loss: 0.0167, BestTest: 0.0127\n",
      "Epoch : 881/1200, train_loss: 0.0229, test_loss: 0.0201, BestTest: 0.0127\n",
      "Epoch : 882/1200, train_loss: 0.0234, test_loss: 0.0191, BestTest: 0.0127\n",
      "Epoch : 883/1200, train_loss: 0.0235, test_loss: 0.0162, BestTest: 0.0127\n",
      "Epoch : 884/1200, train_loss: 0.0229, test_loss: 0.0199, BestTest: 0.0127\n",
      "Epoch : 885/1200, train_loss: 0.0230, test_loss: 0.0208, BestTest: 0.0127\n",
      "Epoch : 886/1200, train_loss: 0.0234, test_loss: 0.0169, BestTest: 0.0127\n",
      "Epoch : 887/1200, train_loss: 0.0234, test_loss: 0.0178, BestTest: 0.0127\n",
      "Epoch : 888/1200, train_loss: 0.0236, test_loss: 0.0190, BestTest: 0.0127\n",
      "Epoch : 889/1200, train_loss: 0.0228, test_loss: 0.0205, BestTest: 0.0127\n",
      "Epoch : 890/1200, train_loss: 0.0228, test_loss: 0.0194, BestTest: 0.0127\n",
      "Epoch : 891/1200, train_loss: 0.0231, test_loss: 0.0176, BestTest: 0.0127\n",
      "Epoch : 892/1200, train_loss: 0.0229, test_loss: 0.0195, BestTest: 0.0127\n",
      "Epoch : 893/1200, train_loss: 0.0240, test_loss: 0.0179, BestTest: 0.0127\n",
      "Epoch : 894/1200, train_loss: 0.0227, test_loss: 0.0163, BestTest: 0.0127\n",
      "Epoch : 895/1200, train_loss: 0.0229, test_loss: 0.0192, BestTest: 0.0127\n",
      "Epoch : 896/1200, train_loss: 0.0233, test_loss: 0.0205, BestTest: 0.0127\n",
      "Epoch : 897/1200, train_loss: 0.0234, test_loss: 0.0223, BestTest: 0.0127\n",
      "Epoch : 898/1200, train_loss: 0.0233, test_loss: 0.0186, BestTest: 0.0127\n",
      "Epoch : 899/1200, train_loss: 0.0234, test_loss: 0.0188, BestTest: 0.0127\n",
      "Epoch : 900/1200, train_loss: 0.0229, test_loss: 0.0190, BestTest: 0.0127\n",
      "Epoch : 901/1200, train_loss: 0.0235, test_loss: 0.0212, BestTest: 0.0127\n",
      "Epoch : 902/1200, train_loss: 0.0232, test_loss: 0.0186, BestTest: 0.0127\n",
      "Epoch : 903/1200, train_loss: 0.0226, test_loss: 0.0194, BestTest: 0.0127\n",
      "Epoch : 904/1200, train_loss: 0.0236, test_loss: 0.0174, BestTest: 0.0127\n",
      "Epoch : 905/1200, train_loss: 0.0226, test_loss: 0.0201, BestTest: 0.0127\n",
      "Epoch : 906/1200, train_loss: 0.0229, test_loss: 0.0176, BestTest: 0.0127\n",
      "Epoch : 907/1200, train_loss: 0.0230, test_loss: 0.0163, BestTest: 0.0127\n",
      "Epoch : 908/1200, train_loss: 0.0223, test_loss: 0.0214, BestTest: 0.0127\n",
      "Epoch : 909/1200, train_loss: 0.0228, test_loss: 0.0209, BestTest: 0.0127\n",
      "Epoch : 910/1200, train_loss: 0.0230, test_loss: 0.0184, BestTest: 0.0127\n",
      "Epoch : 911/1200, train_loss: 0.0226, test_loss: 0.0216, BestTest: 0.0127\n",
      "Epoch : 912/1200, train_loss: 0.0229, test_loss: 0.0203, BestTest: 0.0127\n",
      "Epoch : 913/1200, train_loss: 0.0226, test_loss: 0.0178, BestTest: 0.0127\n",
      "Epoch : 914/1200, train_loss: 0.0229, test_loss: 0.0185, BestTest: 0.0127\n",
      "Epoch : 915/1200, train_loss: 0.0225, test_loss: 0.0182, BestTest: 0.0127\n",
      "Epoch : 916/1200, train_loss: 0.0230, test_loss: 0.0215, BestTest: 0.0127\n",
      "Epoch : 917/1200, train_loss: 0.0229, test_loss: 0.0200, BestTest: 0.0127\n",
      "Epoch : 918/1200, train_loss: 0.0227, test_loss: 0.0169, BestTest: 0.0127\n",
      "Epoch : 919/1200, train_loss: 0.0224, test_loss: 0.0233, BestTest: 0.0127\n",
      "Epoch : 920/1200, train_loss: 0.0226, test_loss: 0.0205, BestTest: 0.0127\n",
      "Epoch : 921/1200, train_loss: 0.0230, test_loss: 0.0187, BestTest: 0.0127\n",
      "Epoch : 922/1200, train_loss: 0.0228, test_loss: 0.0202, BestTest: 0.0127\n",
      "Epoch : 923/1200, train_loss: 0.0228, test_loss: 0.0201, BestTest: 0.0127\n",
      "Epoch : 924/1200, train_loss: 0.0226, test_loss: 0.0198, BestTest: 0.0127\n",
      "Epoch : 925/1200, train_loss: 0.0229, test_loss: 0.0196, BestTest: 0.0127\n",
      "Epoch : 926/1200, train_loss: 0.0227, test_loss: 0.0197, BestTest: 0.0127\n",
      "Epoch : 927/1200, train_loss: 0.0229, test_loss: 0.0200, BestTest: 0.0127\n",
      "Epoch : 928/1200, train_loss: 0.0231, test_loss: 0.0184, BestTest: 0.0127\n",
      "Epoch : 929/1200, train_loss: 0.0227, test_loss: 0.0207, BestTest: 0.0127\n",
      "Epoch : 930/1200, train_loss: 0.0226, test_loss: 0.0166, BestTest: 0.0127\n",
      "Epoch : 931/1200, train_loss: 0.0230, test_loss: 0.0182, BestTest: 0.0127\n",
      "Epoch : 932/1200, train_loss: 0.0230, test_loss: 0.0197, BestTest: 0.0127\n",
      "Epoch : 933/1200, train_loss: 0.0226, test_loss: 0.0173, BestTest: 0.0127\n",
      "Epoch : 934/1200, train_loss: 0.0227, test_loss: 0.0181, BestTest: 0.0127\n",
      "Epoch : 935/1200, train_loss: 0.0228, test_loss: 0.0211, BestTest: 0.0127\n",
      "Epoch : 936/1200, train_loss: 0.0224, test_loss: 0.0208, BestTest: 0.0127\n",
      "Epoch : 937/1200, train_loss: 0.0228, test_loss: 0.0187, BestTest: 0.0127\n",
      "Epoch : 938/1200, train_loss: 0.0222, test_loss: 0.0187, BestTest: 0.0127\n",
      "Epoch : 939/1200, train_loss: 0.0227, test_loss: 0.0215, BestTest: 0.0127\n",
      "Epoch : 940/1200, train_loss: 0.0227, test_loss: 0.0198, BestTest: 0.0127\n",
      "Epoch : 941/1200, train_loss: 0.0231, test_loss: 0.0187, BestTest: 0.0127\n",
      "Epoch : 942/1200, train_loss: 0.0227, test_loss: 0.0191, BestTest: 0.0127\n",
      "Epoch : 943/1200, train_loss: 0.0227, test_loss: 0.0220, BestTest: 0.0127\n",
      "Epoch : 944/1200, train_loss: 0.0226, test_loss: 0.0201, BestTest: 0.0127\n",
      "Epoch : 945/1200, train_loss: 0.0228, test_loss: 0.0215, BestTest: 0.0127\n",
      "Epoch : 946/1200, train_loss: 0.0226, test_loss: 0.0210, BestTest: 0.0127\n",
      "Epoch : 947/1200, train_loss: 0.0228, test_loss: 0.0202, BestTest: 0.0127\n",
      "Epoch : 948/1200, train_loss: 0.0224, test_loss: 0.0206, BestTest: 0.0127\n",
      "Epoch : 949/1200, train_loss: 0.0226, test_loss: 0.0179, BestTest: 0.0127\n",
      "Epoch : 950/1200, train_loss: 0.0232, test_loss: 0.0175, BestTest: 0.0127\n",
      "Epoch : 951/1200, train_loss: 0.0223, test_loss: 0.0199, BestTest: 0.0127\n",
      "Epoch : 952/1200, train_loss: 0.0227, test_loss: 0.0193, BestTest: 0.0127\n",
      "Epoch : 953/1200, train_loss: 0.0227, test_loss: 0.0183, BestTest: 0.0127\n",
      "Epoch : 954/1200, train_loss: 0.0228, test_loss: 0.0179, BestTest: 0.0127\n",
      "Epoch : 955/1200, train_loss: 0.0228, test_loss: 0.0192, BestTest: 0.0127\n",
      "Epoch : 956/1200, train_loss: 0.0226, test_loss: 0.0197, BestTest: 0.0127\n",
      "Epoch : 957/1200, train_loss: 0.0229, test_loss: 0.0211, BestTest: 0.0127\n",
      "Epoch : 958/1200, train_loss: 0.0224, test_loss: 0.0208, BestTest: 0.0127\n",
      "Epoch : 959/1200, train_loss: 0.0225, test_loss: 0.0175, BestTest: 0.0127\n",
      "Epoch : 960/1200, train_loss: 0.0222, test_loss: 0.0188, BestTest: 0.0127\n",
      "Epoch : 961/1200, train_loss: 0.0225, test_loss: 0.0174, BestTest: 0.0127\n",
      "Epoch : 962/1200, train_loss: 0.0231, test_loss: 0.0185, BestTest: 0.0127\n",
      "Epoch : 963/1200, train_loss: 0.0226, test_loss: 0.0191, BestTest: 0.0127\n",
      "Epoch : 964/1200, train_loss: 0.0225, test_loss: 0.0203, BestTest: 0.0127\n",
      "Epoch : 965/1200, train_loss: 0.0229, test_loss: 0.0217, BestTest: 0.0127\n",
      "Epoch : 966/1200, train_loss: 0.0230, test_loss: 0.0193, BestTest: 0.0127\n",
      "Epoch : 967/1200, train_loss: 0.0229, test_loss: 0.0215, BestTest: 0.0127\n",
      "Epoch : 968/1200, train_loss: 0.0225, test_loss: 0.0172, BestTest: 0.0127\n",
      "Epoch : 969/1200, train_loss: 0.0229, test_loss: 0.0196, BestTest: 0.0127\n",
      "Epoch : 970/1200, train_loss: 0.0228, test_loss: 0.0222, BestTest: 0.0127\n",
      "Epoch : 971/1200, train_loss: 0.0230, test_loss: 0.0169, BestTest: 0.0127\n",
      "Epoch : 972/1200, train_loss: 0.0227, test_loss: 0.0182, BestTest: 0.0127\n",
      "Epoch : 973/1200, train_loss: 0.0228, test_loss: 0.0183, BestTest: 0.0127\n",
      "Epoch : 974/1200, train_loss: 0.0227, test_loss: 0.0214, BestTest: 0.0127\n",
      "Epoch : 975/1200, train_loss: 0.0226, test_loss: 0.0189, BestTest: 0.0127\n",
      "Epoch : 976/1200, train_loss: 0.0233, test_loss: 0.0166, BestTest: 0.0127\n",
      "Epoch : 977/1200, train_loss: 0.0227, test_loss: 0.0176, BestTest: 0.0127\n",
      "Epoch : 978/1200, train_loss: 0.0226, test_loss: 0.0187, BestTest: 0.0127\n",
      "Epoch : 979/1200, train_loss: 0.0229, test_loss: 0.0175, BestTest: 0.0127\n",
      "Epoch : 980/1200, train_loss: 0.0225, test_loss: 0.0190, BestTest: 0.0127\n",
      "Epoch : 981/1200, train_loss: 0.0223, test_loss: 0.0176, BestTest: 0.0127\n",
      "Epoch : 982/1200, train_loss: 0.0228, test_loss: 0.0196, BestTest: 0.0127\n",
      "Epoch : 983/1200, train_loss: 0.0227, test_loss: 0.0189, BestTest: 0.0127\n",
      "Epoch : 984/1200, train_loss: 0.0224, test_loss: 0.0208, BestTest: 0.0127\n",
      "Epoch : 985/1200, train_loss: 0.0226, test_loss: 0.0228, BestTest: 0.0127\n",
      "Epoch : 986/1200, train_loss: 0.0229, test_loss: 0.0200, BestTest: 0.0127\n",
      "Epoch : 987/1200, train_loss: 0.0225, test_loss: 0.0198, BestTest: 0.0127\n",
      "Epoch : 988/1200, train_loss: 0.0230, test_loss: 0.0180, BestTest: 0.0127\n",
      "Epoch : 989/1200, train_loss: 0.0225, test_loss: 0.0201, BestTest: 0.0127\n",
      "Epoch : 990/1200, train_loss: 0.0227, test_loss: 0.0175, BestTest: 0.0127\n",
      "Epoch : 991/1200, train_loss: 0.0230, test_loss: 0.0187, BestTest: 0.0127\n",
      "Epoch : 992/1200, train_loss: 0.0222, test_loss: 0.0190, BestTest: 0.0127\n",
      "Epoch : 993/1200, train_loss: 0.0226, test_loss: 0.0176, BestTest: 0.0127\n",
      "Epoch : 994/1200, train_loss: 0.0227, test_loss: 0.0178, BestTest: 0.0127\n",
      "Epoch : 995/1200, train_loss: 0.0230, test_loss: 0.0202, BestTest: 0.0127\n",
      "Epoch : 996/1200, train_loss: 0.0230, test_loss: 0.0167, BestTest: 0.0127\n",
      "Epoch : 997/1200, train_loss: 0.0227, test_loss: 0.0185, BestTest: 0.0127\n",
      "Epoch : 998/1200, train_loss: 0.0226, test_loss: 0.0194, BestTest: 0.0127\n",
      "Epoch : 999/1200, train_loss: 0.0224, test_loss: 0.0226, BestTest: 0.0127\n",
      "Epoch : 1000/1200, train_loss: 0.0223, test_loss: 0.0196, BestTest: 0.0127\n",
      "Epoch : 1001/1200, train_loss: 0.0222, test_loss: 0.0220, BestTest: 0.0127\n",
      "Epoch : 1002/1200, train_loss: 0.0227, test_loss: 0.0195, BestTest: 0.0127\n",
      "Epoch : 1003/1200, train_loss: 0.0226, test_loss: 0.0211, BestTest: 0.0127\n",
      "Epoch : 1004/1200, train_loss: 0.0218, test_loss: 0.0204, BestTest: 0.0127\n",
      "Epoch : 1005/1200, train_loss: 0.0225, test_loss: 0.0205, BestTest: 0.0127\n",
      "Epoch : 1006/1200, train_loss: 0.0224, test_loss: 0.0207, BestTest: 0.0127\n",
      "Epoch : 1007/1200, train_loss: 0.0221, test_loss: 0.0166, BestTest: 0.0127\n",
      "Epoch : 1008/1200, train_loss: 0.0222, test_loss: 0.0190, BestTest: 0.0127\n",
      "Epoch : 1009/1200, train_loss: 0.0225, test_loss: 0.0214, BestTest: 0.0127\n",
      "Epoch : 1010/1200, train_loss: 0.0226, test_loss: 0.0207, BestTest: 0.0127\n",
      "Epoch : 1011/1200, train_loss: 0.0226, test_loss: 0.0205, BestTest: 0.0127\n",
      "Epoch : 1012/1200, train_loss: 0.0224, test_loss: 0.0194, BestTest: 0.0127\n",
      "Epoch : 1013/1200, train_loss: 0.0224, test_loss: 0.0183, BestTest: 0.0127\n",
      "Epoch : 1014/1200, train_loss: 0.0230, test_loss: 0.0236, BestTest: 0.0127\n",
      "Epoch : 1015/1200, train_loss: 0.0222, test_loss: 0.0192, BestTest: 0.0127\n",
      "Epoch : 1016/1200, train_loss: 0.0222, test_loss: 0.0216, BestTest: 0.0127\n",
      "Epoch : 1017/1200, train_loss: 0.0226, test_loss: 0.0173, BestTest: 0.0127\n",
      "Epoch : 1018/1200, train_loss: 0.0226, test_loss: 0.0174, BestTest: 0.0127\n",
      "Epoch : 1019/1200, train_loss: 0.0224, test_loss: 0.0179, BestTest: 0.0127\n",
      "Epoch : 1020/1200, train_loss: 0.0223, test_loss: 0.0162, BestTest: 0.0127\n",
      "Epoch : 1021/1200, train_loss: 0.0225, test_loss: 0.0167, BestTest: 0.0127\n",
      "Epoch : 1022/1200, train_loss: 0.0225, test_loss: 0.0188, BestTest: 0.0127\n",
      "Epoch : 1023/1200, train_loss: 0.0229, test_loss: 0.0209, BestTest: 0.0127\n",
      "Epoch : 1024/1200, train_loss: 0.0227, test_loss: 0.0187, BestTest: 0.0127\n",
      "Epoch : 1025/1200, train_loss: 0.0225, test_loss: 0.0196, BestTest: 0.0127\n",
      "Epoch : 1026/1200, train_loss: 0.0223, test_loss: 0.0187, BestTest: 0.0127\n",
      "Epoch : 1027/1200, train_loss: 0.0221, test_loss: 0.0166, BestTest: 0.0127\n",
      "Epoch : 1028/1200, train_loss: 0.0231, test_loss: 0.0207, BestTest: 0.0127\n",
      "Epoch : 1029/1200, train_loss: 0.0227, test_loss: 0.0177, BestTest: 0.0127\n",
      "Epoch : 1030/1200, train_loss: 0.0223, test_loss: 0.0201, BestTest: 0.0127\n",
      "Epoch : 1031/1200, train_loss: 0.0226, test_loss: 0.0211, BestTest: 0.0127\n",
      "Epoch : 1032/1200, train_loss: 0.0222, test_loss: 0.0175, BestTest: 0.0127\n",
      "Epoch : 1033/1200, train_loss: 0.0222, test_loss: 0.0182, BestTest: 0.0127\n",
      "Epoch : 1034/1200, train_loss: 0.0226, test_loss: 0.0182, BestTest: 0.0127\n",
      "Epoch : 1035/1200, train_loss: 0.0224, test_loss: 0.0212, BestTest: 0.0127\n",
      "Epoch : 1036/1200, train_loss: 0.0223, test_loss: 0.0174, BestTest: 0.0127\n",
      "Epoch : 1037/1200, train_loss: 0.0225, test_loss: 0.0201, BestTest: 0.0127\n",
      "Epoch : 1038/1200, train_loss: 0.0227, test_loss: 0.0211, BestTest: 0.0127\n",
      "Epoch : 1039/1200, train_loss: 0.0224, test_loss: 0.0219, BestTest: 0.0127\n",
      "Epoch : 1040/1200, train_loss: 0.0226, test_loss: 0.0199, BestTest: 0.0127\n",
      "Epoch : 1041/1200, train_loss: 0.0223, test_loss: 0.0197, BestTest: 0.0127\n",
      "Epoch : 1042/1200, train_loss: 0.0223, test_loss: 0.0226, BestTest: 0.0127\n",
      "Epoch : 1043/1200, train_loss: 0.0227, test_loss: 0.0208, BestTest: 0.0127\n",
      "Epoch : 1044/1200, train_loss: 0.0230, test_loss: 0.0207, BestTest: 0.0127\n",
      "Epoch : 1045/1200, train_loss: 0.0225, test_loss: 0.0204, BestTest: 0.0127\n",
      "Epoch : 1046/1200, train_loss: 0.0225, test_loss: 0.0188, BestTest: 0.0127\n",
      "Epoch : 1047/1200, train_loss: 0.0225, test_loss: 0.0177, BestTest: 0.0127\n",
      "Epoch : 1048/1200, train_loss: 0.0230, test_loss: 0.0185, BestTest: 0.0127\n",
      "Epoch : 1049/1200, train_loss: 0.0224, test_loss: 0.0187, BestTest: 0.0127\n",
      "Epoch : 1050/1200, train_loss: 0.0223, test_loss: 0.0179, BestTest: 0.0127\n",
      "Epoch : 1051/1200, train_loss: 0.0222, test_loss: 0.0203, BestTest: 0.0127\n",
      "Epoch : 1052/1200, train_loss: 0.0222, test_loss: 0.0171, BestTest: 0.0127\n",
      "Epoch : 1053/1200, train_loss: 0.0221, test_loss: 0.0205, BestTest: 0.0127\n",
      "Epoch : 1054/1200, train_loss: 0.0224, test_loss: 0.0203, BestTest: 0.0127\n",
      "Epoch : 1055/1200, train_loss: 0.0225, test_loss: 0.0180, BestTest: 0.0127\n",
      "Epoch : 1056/1200, train_loss: 0.0224, test_loss: 0.0185, BestTest: 0.0127\n",
      "Epoch : 1057/1200, train_loss: 0.0223, test_loss: 0.0210, BestTest: 0.0127\n",
      "Epoch : 1058/1200, train_loss: 0.0227, test_loss: 0.0186, BestTest: 0.0127\n",
      "Epoch : 1059/1200, train_loss: 0.0226, test_loss: 0.0175, BestTest: 0.0127\n",
      "Epoch : 1060/1200, train_loss: 0.0225, test_loss: 0.0190, BestTest: 0.0127\n",
      "Epoch : 1061/1200, train_loss: 0.0222, test_loss: 0.0170, BestTest: 0.0127\n",
      "Epoch : 1062/1200, train_loss: 0.0224, test_loss: 0.0208, BestTest: 0.0127\n",
      "Epoch : 1063/1200, train_loss: 0.0228, test_loss: 0.0205, BestTest: 0.0127\n",
      "Epoch : 1064/1200, train_loss: 0.0229, test_loss: 0.0187, BestTest: 0.0127\n",
      "Epoch : 1065/1200, train_loss: 0.0221, test_loss: 0.0167, BestTest: 0.0127\n",
      "Epoch : 1066/1200, train_loss: 0.0221, test_loss: 0.0184, BestTest: 0.0127\n",
      "Epoch : 1067/1200, train_loss: 0.0225, test_loss: 0.0172, BestTest: 0.0127\n",
      "Epoch : 1068/1200, train_loss: 0.0226, test_loss: 0.0178, BestTest: 0.0127\n",
      "Epoch : 1069/1200, train_loss: 0.0229, test_loss: 0.0206, BestTest: 0.0127\n",
      "Epoch : 1070/1200, train_loss: 0.0225, test_loss: 0.0176, BestTest: 0.0127\n",
      "Epoch : 1071/1200, train_loss: 0.0224, test_loss: 0.0189, BestTest: 0.0127\n",
      "Epoch : 1072/1200, train_loss: 0.0221, test_loss: 0.0185, BestTest: 0.0127\n",
      "Epoch : 1073/1200, train_loss: 0.0230, test_loss: 0.0208, BestTest: 0.0127\n",
      "Epoch : 1074/1200, train_loss: 0.0221, test_loss: 0.0183, BestTest: 0.0127\n",
      "Epoch : 1075/1200, train_loss: 0.0224, test_loss: 0.0182, BestTest: 0.0127\n",
      "Epoch : 1076/1200, train_loss: 0.0225, test_loss: 0.0211, BestTest: 0.0127\n",
      "Epoch : 1077/1200, train_loss: 0.0227, test_loss: 0.0205, BestTest: 0.0127\n",
      "Epoch : 1078/1200, train_loss: 0.0227, test_loss: 0.0175, BestTest: 0.0127\n",
      "Epoch : 1079/1200, train_loss: 0.0224, test_loss: 0.0170, BestTest: 0.0127\n",
      "Epoch : 1080/1200, train_loss: 0.0225, test_loss: 0.0199, BestTest: 0.0127\n",
      "Epoch : 1081/1200, train_loss: 0.0225, test_loss: 0.0204, BestTest: 0.0127\n",
      "Epoch : 1082/1200, train_loss: 0.0225, test_loss: 0.0184, BestTest: 0.0127\n",
      "Epoch : 1083/1200, train_loss: 0.0225, test_loss: 0.0185, BestTest: 0.0127\n",
      "Epoch : 1084/1200, train_loss: 0.0226, test_loss: 0.0220, BestTest: 0.0127\n",
      "Epoch : 1085/1200, train_loss: 0.0225, test_loss: 0.0225, BestTest: 0.0127\n",
      "Epoch : 1086/1200, train_loss: 0.0219, test_loss: 0.0192, BestTest: 0.0127\n",
      "Epoch : 1087/1200, train_loss: 0.0227, test_loss: 0.0171, BestTest: 0.0127\n",
      "Epoch : 1088/1200, train_loss: 0.0226, test_loss: 0.0190, BestTest: 0.0127\n",
      "Epoch : 1089/1200, train_loss: 0.0226, test_loss: 0.0183, BestTest: 0.0127\n",
      "Epoch : 1090/1200, train_loss: 0.0225, test_loss: 0.0193, BestTest: 0.0127\n",
      "Epoch : 1091/1200, train_loss: 0.0220, test_loss: 0.0187, BestTest: 0.0127\n",
      "Epoch : 1092/1200, train_loss: 0.0224, test_loss: 0.0168, BestTest: 0.0127\n",
      "Epoch : 1093/1200, train_loss: 0.0227, test_loss: 0.0171, BestTest: 0.0127\n",
      "Epoch : 1094/1200, train_loss: 0.0224, test_loss: 0.0191, BestTest: 0.0127\n",
      "Epoch : 1095/1200, train_loss: 0.0222, test_loss: 0.0197, BestTest: 0.0127\n",
      "Epoch : 1096/1200, train_loss: 0.0226, test_loss: 0.0215, BestTest: 0.0127\n",
      "Epoch : 1097/1200, train_loss: 0.0223, test_loss: 0.0186, BestTest: 0.0127\n",
      "Epoch : 1098/1200, train_loss: 0.0227, test_loss: 0.0194, BestTest: 0.0127\n",
      "Epoch : 1099/1200, train_loss: 0.0228, test_loss: 0.0198, BestTest: 0.0127\n",
      "Epoch : 1100/1200, train_loss: 0.0219, test_loss: 0.0179, BestTest: 0.0127\n",
      "Epoch : 1101/1200, train_loss: 0.0221, test_loss: 0.0173, BestTest: 0.0127\n",
      "Epoch : 1102/1200, train_loss: 0.0222, test_loss: 0.0195, BestTest: 0.0127\n",
      "Epoch : 1103/1200, train_loss: 0.0227, test_loss: 0.0200, BestTest: 0.0127\n",
      "Epoch : 1104/1200, train_loss: 0.0225, test_loss: 0.0173, BestTest: 0.0127\n",
      "Epoch : 1105/1200, train_loss: 0.0221, test_loss: 0.0177, BestTest: 0.0127\n",
      "Epoch : 1106/1200, train_loss: 0.0225, test_loss: 0.0197, BestTest: 0.0127\n",
      "Epoch : 1107/1200, train_loss: 0.0220, test_loss: 0.0212, BestTest: 0.0127\n",
      "Epoch : 1108/1200, train_loss: 0.0225, test_loss: 0.0194, BestTest: 0.0127\n",
      "Epoch : 1109/1200, train_loss: 0.0224, test_loss: 0.0164, BestTest: 0.0127\n",
      "Epoch : 1110/1200, train_loss: 0.0226, test_loss: 0.0217, BestTest: 0.0127\n",
      "Epoch : 1111/1200, train_loss: 0.0226, test_loss: 0.0174, BestTest: 0.0127\n",
      "Epoch : 1112/1200, train_loss: 0.0223, test_loss: 0.0199, BestTest: 0.0127\n",
      "Epoch : 1113/1200, train_loss: 0.0224, test_loss: 0.0195, BestTest: 0.0127\n",
      "Epoch : 1114/1200, train_loss: 0.0226, test_loss: 0.0215, BestTest: 0.0127\n",
      "Epoch : 1115/1200, train_loss: 0.0223, test_loss: 0.0170, BestTest: 0.0127\n",
      "Epoch : 1116/1200, train_loss: 0.0224, test_loss: 0.0197, BestTest: 0.0127\n",
      "Epoch : 1117/1200, train_loss: 0.0225, test_loss: 0.0206, BestTest: 0.0127\n",
      "Epoch : 1118/1200, train_loss: 0.0222, test_loss: 0.0169, BestTest: 0.0127\n",
      "Epoch : 1119/1200, train_loss: 0.0227, test_loss: 0.0226, BestTest: 0.0127\n",
      "Epoch : 1120/1200, train_loss: 0.0221, test_loss: 0.0199, BestTest: 0.0127\n",
      "Epoch : 1121/1200, train_loss: 0.0225, test_loss: 0.0197, BestTest: 0.0127\n",
      "Epoch : 1122/1200, train_loss: 0.0223, test_loss: 0.0187, BestTest: 0.0127\n",
      "Epoch : 1123/1200, train_loss: 0.0223, test_loss: 0.0169, BestTest: 0.0127\n",
      "Epoch : 1124/1200, train_loss: 0.0221, test_loss: 0.0213, BestTest: 0.0127\n",
      "Epoch : 1125/1200, train_loss: 0.0232, test_loss: 0.0180, BestTest: 0.0127\n",
      "Epoch : 1126/1200, train_loss: 0.0226, test_loss: 0.0199, BestTest: 0.0127\n",
      "Epoch : 1127/1200, train_loss: 0.0221, test_loss: 0.0174, BestTest: 0.0127\n",
      "Epoch : 1128/1200, train_loss: 0.0223, test_loss: 0.0217, BestTest: 0.0127\n",
      "Epoch : 1129/1200, train_loss: 0.0228, test_loss: 0.0199, BestTest: 0.0127\n",
      "Epoch : 1130/1200, train_loss: 0.0224, test_loss: 0.0195, BestTest: 0.0127\n",
      "Epoch : 1131/1200, train_loss: 0.0222, test_loss: 0.0177, BestTest: 0.0127\n",
      "Epoch : 1132/1200, train_loss: 0.0219, test_loss: 0.0157, BestTest: 0.0127\n",
      "Epoch : 1133/1200, train_loss: 0.0221, test_loss: 0.0178, BestTest: 0.0127\n",
      "Epoch : 1134/1200, train_loss: 0.0225, test_loss: 0.0202, BestTest: 0.0127\n",
      "Epoch : 1135/1200, train_loss: 0.0225, test_loss: 0.0178, BestTest: 0.0127\n",
      "Epoch : 1136/1200, train_loss: 0.0221, test_loss: 0.0192, BestTest: 0.0127\n",
      "Epoch : 1137/1200, train_loss: 0.0226, test_loss: 0.0196, BestTest: 0.0127\n",
      "Epoch : 1138/1200, train_loss: 0.0227, test_loss: 0.0217, BestTest: 0.0127\n",
      "Epoch : 1139/1200, train_loss: 0.0226, test_loss: 0.0211, BestTest: 0.0127\n",
      "Epoch : 1140/1200, train_loss: 0.0222, test_loss: 0.0195, BestTest: 0.0127\n",
      "Epoch : 1141/1200, train_loss: 0.0224, test_loss: 0.0173, BestTest: 0.0127\n",
      "Epoch : 1142/1200, train_loss: 0.0222, test_loss: 0.0189, BestTest: 0.0127\n",
      "Epoch : 1143/1200, train_loss: 0.0220, test_loss: 0.0197, BestTest: 0.0127\n",
      "Epoch : 1144/1200, train_loss: 0.0222, test_loss: 0.0165, BestTest: 0.0127\n",
      "Epoch : 1145/1200, train_loss: 0.0225, test_loss: 0.0208, BestTest: 0.0127\n",
      "Epoch : 1146/1200, train_loss: 0.0226, test_loss: 0.0206, BestTest: 0.0127\n",
      "Epoch : 1147/1200, train_loss: 0.0225, test_loss: 0.0188, BestTest: 0.0127\n",
      "Epoch : 1148/1200, train_loss: 0.0225, test_loss: 0.0206, BestTest: 0.0127\n",
      "Epoch : 1149/1200, train_loss: 0.0226, test_loss: 0.0203, BestTest: 0.0127\n",
      "Epoch : 1150/1200, train_loss: 0.0223, test_loss: 0.0186, BestTest: 0.0127\n",
      "Epoch : 1151/1200, train_loss: 0.0218, test_loss: 0.0174, BestTest: 0.0127\n",
      "Epoch : 1152/1200, train_loss: 0.0223, test_loss: 0.0199, BestTest: 0.0127\n",
      "Epoch : 1153/1200, train_loss: 0.0225, test_loss: 0.0192, BestTest: 0.0127\n",
      "Epoch : 1154/1200, train_loss: 0.0233, test_loss: 0.0189, BestTest: 0.0127\n",
      "Epoch : 1155/1200, train_loss: 0.0226, test_loss: 0.0196, BestTest: 0.0127\n",
      "Epoch : 1156/1200, train_loss: 0.0226, test_loss: 0.0203, BestTest: 0.0127\n",
      "Epoch : 1157/1200, train_loss: 0.0220, test_loss: 0.0186, BestTest: 0.0127\n",
      "Epoch : 1158/1200, train_loss: 0.0227, test_loss: 0.0187, BestTest: 0.0127\n",
      "Epoch : 1159/1200, train_loss: 0.0224, test_loss: 0.0196, BestTest: 0.0127\n",
      "Epoch : 1160/1200, train_loss: 0.0227, test_loss: 0.0212, BestTest: 0.0127\n",
      "Epoch : 1161/1200, train_loss: 0.0226, test_loss: 0.0185, BestTest: 0.0127\n",
      "Epoch : 1162/1200, train_loss: 0.0225, test_loss: 0.0195, BestTest: 0.0127\n",
      "Epoch : 1163/1200, train_loss: 0.0222, test_loss: 0.0192, BestTest: 0.0127\n",
      "Epoch : 1164/1200, train_loss: 0.0227, test_loss: 0.0211, BestTest: 0.0127\n",
      "Epoch : 1165/1200, train_loss: 0.0221, test_loss: 0.0181, BestTest: 0.0127\n",
      "Epoch : 1166/1200, train_loss: 0.0227, test_loss: 0.0175, BestTest: 0.0127\n",
      "Epoch : 1167/1200, train_loss: 0.0221, test_loss: 0.0199, BestTest: 0.0127\n",
      "Epoch : 1168/1200, train_loss: 0.0224, test_loss: 0.0177, BestTest: 0.0127\n",
      "Epoch : 1169/1200, train_loss: 0.0225, test_loss: 0.0225, BestTest: 0.0127\n",
      "Epoch : 1170/1200, train_loss: 0.0226, test_loss: 0.0179, BestTest: 0.0127\n",
      "Epoch : 1171/1200, train_loss: 0.0227, test_loss: 0.0186, BestTest: 0.0127\n",
      "Epoch : 1172/1200, train_loss: 0.0223, test_loss: 0.0217, BestTest: 0.0127\n",
      "Epoch : 1173/1200, train_loss: 0.0219, test_loss: 0.0190, BestTest: 0.0127\n",
      "Epoch : 1174/1200, train_loss: 0.0223, test_loss: 0.0178, BestTest: 0.0127\n",
      "Epoch : 1175/1200, train_loss: 0.0222, test_loss: 0.0181, BestTest: 0.0127\n",
      "Epoch : 1176/1200, train_loss: 0.0222, test_loss: 0.0193, BestTest: 0.0127\n",
      "Epoch : 1177/1200, train_loss: 0.0228, test_loss: 0.0228, BestTest: 0.0127\n",
      "Epoch : 1178/1200, train_loss: 0.0224, test_loss: 0.0180, BestTest: 0.0127\n",
      "Epoch : 1179/1200, train_loss: 0.0221, test_loss: 0.0191, BestTest: 0.0127\n",
      "Epoch : 1180/1200, train_loss: 0.0221, test_loss: 0.0190, BestTest: 0.0127\n",
      "Epoch : 1181/1200, train_loss: 0.0226, test_loss: 0.0200, BestTest: 0.0127\n",
      "Epoch : 1182/1200, train_loss: 0.0226, test_loss: 0.0204, BestTest: 0.0127\n",
      "Epoch : 1183/1200, train_loss: 0.0226, test_loss: 0.0169, BestTest: 0.0127\n",
      "Epoch : 1184/1200, train_loss: 0.0226, test_loss: 0.0193, BestTest: 0.0127\n",
      "Epoch : 1185/1200, train_loss: 0.0225, test_loss: 0.0189, BestTest: 0.0127\n",
      "Epoch : 1186/1200, train_loss: 0.0224, test_loss: 0.0177, BestTest: 0.0127\n",
      "Epoch : 1187/1200, train_loss: 0.0220, test_loss: 0.0176, BestTest: 0.0127\n",
      "Epoch : 1188/1200, train_loss: 0.0226, test_loss: 0.0205, BestTest: 0.0127\n",
      "Epoch : 1189/1200, train_loss: 0.0225, test_loss: 0.0186, BestTest: 0.0127\n",
      "Epoch : 1190/1200, train_loss: 0.0221, test_loss: 0.0172, BestTest: 0.0127\n",
      "Epoch : 1191/1200, train_loss: 0.0223, test_loss: 0.0201, BestTest: 0.0127\n",
      "Epoch : 1192/1200, train_loss: 0.0223, test_loss: 0.0180, BestTest: 0.0127\n",
      "Epoch : 1193/1200, train_loss: 0.0226, test_loss: 0.0195, BestTest: 0.0127\n",
      "Epoch : 1194/1200, train_loss: 0.0227, test_loss: 0.0163, BestTest: 0.0127\n",
      "Epoch : 1195/1200, train_loss: 0.0222, test_loss: 0.0170, BestTest: 0.0127\n",
      "Epoch : 1196/1200, train_loss: 0.0219, test_loss: 0.0195, BestTest: 0.0127\n",
      "Epoch : 1197/1200, train_loss: 0.0221, test_loss: 0.0199, BestTest: 0.0127\n",
      "Epoch : 1198/1200, train_loss: 0.0223, test_loss: 0.0208, BestTest: 0.0127\n",
      "Epoch : 1199/1200, train_loss: 0.0220, test_loss: 0.0196, BestTest: 0.0127\n",
      "Epoch : 1200/1200, train_loss: 0.0224, test_loss: 0.0212, BestTest: 0.0127\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\t\n",
    "\tfile_name1 = '/hy-tmp/data_unzip/data/Case_1_2_Training.npy'\n",
    "\t#print('The trainX dataset is : %s'%(file_name1))\n",
    "\tCIR = np.load(file_name1)\n",
    "\ttrainX = CIR.transpose((2,1,3,0))  #[none, 256, 72, 2]\n",
    "\t\n",
    "\tfile_name2 = '/hy-tmp/data_unzip/data/Case_1_2_Training_Label.npy'\n",
    "\t#print('The trainY dataset is : %s'%(file_name2))\n",
    "\tPOS = np.load(file_name2)\n",
    "\ttrainY = POS.transpose((1,0)) #[none, 2]\n",
    "\t\n",
    "\tmodel = Model_1(no_grad=False)\n",
    "\tmodel = model.to(DEVICE)\n",
    "\tprint(model)\n",
    "\t\n",
    "\ttrain_dataset = MyDataset(trainX, trainY, split_ratio, mode='train')\n",
    "\ttrain_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\t\n",
    "\tval_dataset = MyDataset(trainX, trainY, split_ratio, mode='val')\n",
    "\tval_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\t\n",
    "\t\n",
    "\tcriterion = nn.L1Loss().to(DEVICE)\n",
    "\t\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\t\n",
    "\t\n",
    "\ttest_avg_min = 10000\n",
    "\t\n",
    "\tprint('开始训练------')\n",
    "\tfor epoch in range(TOTAL_EPOCHS):\n",
    "\t\tmodel.train()\n",
    "\t\t#optimizer.param_groups[0]['lr'] = LEARNING_RATE /np.sqrt(np.sqrt(epoch+1))\n",
    "\t\t\n",
    "\t\t# Learning rate decay\n",
    "\t\tif (epoch + 1) % change_learning_rate_epochs == 0:\n",
    "\t\t\toptimizer.param_groups[0]['lr'] /= 2 \n",
    "\t\t\t#print('lr:%.4e' % optimizer.param_groups[0]['lr'])\n",
    "\t\t\n",
    "\t\t#Training in this epoch  \n",
    "\t\tloss_avg = 0\n",
    "\t\tfor i, (x, y) in enumerate(train_loader):\n",
    "\t\t\tx = x.float().to(DEVICE)\n",
    "\t\t\ty = y.float().to(DEVICE)\n",
    "\t\t\t\n",
    "\t\t\t# 清零\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\toutput = model(x)\n",
    "\t\t\t# 计算损失函数\n",
    "\t\t\tloss = criterion(output, y)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\t\n",
    "\t\t\tloss_avg += loss.item() \n",
    "\t\t\t\n",
    "\t\tloss_avg /= len(train_loader)\n",
    "\t\t\n",
    "\t\t#Testing in this epoch\n",
    "\t\tmodel.eval()\n",
    "\t\ttest_avg = 0\n",
    "\t\tfor i, (x, y) in enumerate(val_loader):\n",
    "\t\t\tx = x.float().to(DEVICE)\n",
    "\t\t\ty = y.float().to(DEVICE)\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\toutput = model(x)\n",
    "\t\t\t\t# 计算损失函数\n",
    "\t\t\t\tloss_test = criterion(output, y)\n",
    "\t\t\t\ttest_avg += loss_test.item() \n",
    "\t\t\n",
    "\t\ttest_avg /= len(val_loader)\n",
    "\t\t\n",
    "\t\t#保存模型参数\n",
    "\t\tif test_avg < test_avg_min:\n",
    "\t\t\t#print('Model saved!')\n",
    "\t\t\ttest_avg_min = test_avg\n",
    "\t\t\t\n",
    "\t\t\ttorch.save(model.state_dict(), model_save)\n",
    "\t\t\n",
    "\t\tprint('Epoch : %d/%d, train_loss: %.4f, test_loss: %.4f, BestTest: %.4f' % (epoch + 1, TOTAL_EPOCHS, loss_avg, test_avg, test_avg_min))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86ba92-3b60-40f7-b213-854f4e50bbb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
